{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_b39R9Z6JYG"
      },
      "source": [
        "# **Building The Research Agent**\n",
        "\n",
        "We will be working towards building an agent in a step by step manner. Anthropic released a blog post about build AI Agents and discussed some very important concepts - [Building Effective Agents by Anthropic]( https://www.anthropic.com/engineering/building-effective-agents). We will walk through some critical patterns that help us to build the agent.\n",
        "\n",
        "Following are the patterns we will explore:\n",
        "1. The Agumented Model\n",
        "2. Prompt Chaining\n",
        "3. Routing\n",
        "4. Parallelization - **Not Implemented**\n",
        "5. Orchestrator-workers - **Not Implemented**\n",
        "6. Evaluator-optimizer - **Not Implemented**\n",
        "7. The Agent\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h8Z03BaA7Dt"
      },
      "source": [
        "## The Augmented Model\n",
        "\n",
        "![Anthropic Augmented LLM](https://drive.google.com/uc?export=view&id=1FDId1KGkJB3whzXVLvsW0xPghxVmQ9w2)\n",
        "This is the fundamental building block for building an Agents. The LLM is supplimented with retrieval, tools and memory.\n",
        "\n",
        "Let us build a simple version of this tool.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MZwaRkbVcNw"
      },
      "source": [
        "#### Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yBJ5ziJmVYp0",
        "outputId": "1e46a625-d30d-48ef-b6eb-8ceed393c86a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.24-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-0.3.56-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.34-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: requests in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (2.32.3)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: python-dotenv in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (1.0.0)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.3.38-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
            "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading sqlalchemy-2.0.40-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain-core) (9.0.0)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain-core) (4.12.2)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.23.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.25-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.8 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.63-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from requests) (2024.12.14)\n",
            "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\n",
            "  Downloading openai-1.76.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.2)\n",
            "Requirement already satisfied: sniffio in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (43 kB)\n",
            "Collecting orjson>=3.10.1 (from langgraph-sdk<0.2.0,>=0.1.42->langgraph)\n",
            "  Downloading orjson-3.10.16-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain-openai)\n",
            "  Downloading jiter-0.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: tqdm>4 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading pydantic_core-2.33.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Downloading langchain-0.3.24-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.56-py3-none-any.whl (437 kB)\n",
            "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading langgraph-0.3.34-py3-none-any.whl (148 kB)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "Downloading langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "Downloading groq-0.23.1-py3-none-any.whl (127 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
            "Downloading langgraph_checkpoint-2.0.25-py3-none-any.whl (42 kB)\n",
            "Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.63-py3-none-any.whl (47 kB)\n",
            "Downloading langsmith-0.3.38-py3-none-any.whl (359 kB)\n",
            "Downloading openai-1.76.0-py3-none-any.whl (661 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
            "Downloading pydantic_core-2.33.1-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.40-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
            "Downloading jiter-0.9.0-cp311-cp311-macosx_11_0_arm64.whl (320 kB)\n",
            "Downloading orjson-3.10.16-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
            "Downloading ormsgpack-1.9.1-cp311-cp311-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (382 kB)\n",
            "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl (633 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m633.7/633.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=834332c7d8566703bc5ea36677184e75dd7c8918c0a96e1d7fa64a580334dcb4\n",
            "  Stored in directory: /Users/tchavva/Library/Caches/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, zstandard, xxhash, typing-inspection, SQLAlchemy, PyPDF2, pydantic-core, ormsgpack, orjson, jsonpatch, jiter, feedparser, tiktoken, requests-toolbelt, pydantic, arxiv, openai, langsmith, langgraph-sdk, groq, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langchain-groq, langgraph-prebuilt, langchain, langgraph\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.14.5\n",
            "    Uninstalling pydantic_core-2.14.5:\n",
            "      Successfully uninstalled pydantic_core-2.14.5\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.5.2\n",
            "    Uninstalling pydantic-2.5.2:\n",
            "      Successfully uninstalled pydantic-2.5.2\n",
            "Successfully installed PyPDF2-3.0.1 SQLAlchemy-2.0.40 arxiv-2.2.0 feedparser-6.0.11 groq-0.23.1 jiter-0.9.0 jsonpatch-1.33 langchain-0.3.24 langchain-core-0.3.56 langchain-groq-0.3.2 langchain-openai-0.3.14 langchain-text-splitters-0.3.8 langgraph-0.3.34 langgraph-checkpoint-2.0.25 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.63 langsmith-0.3.38 openai-1.76.0 orjson-3.10.16 ormsgpack-1.9.1 pydantic-2.11.3 pydantic-core-2.33.1 requests-toolbelt-1.0.0 sgmllib3k-1.0.0 tiktoken-0.9.0 typing-inspection-0.4.0 xxhash-3.5.0 zstandard-0.23.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-core langchain-groq langgraph arxiv PyPDF2 requests langchain-openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmeBWd33Vekn"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rPPjuB-tVg7M"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from typing import List, Dict, Any, Optional\n",
        "import arxiv\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import END, StateGraph\n",
        "#from langgraph.prebuilt import ToolExecutor\n",
        "import requests\n",
        "import PyPDF2\n",
        "from io import BytesIO\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYkvzwi2Z4Ct",
        "outputId": "721f08f7-e09a-44c6-91fe-a1875706fe23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq API key loaded successfully!\n",
            "OpenAI API key loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get the Groq API key\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Check if the key was found\n",
        "if not groq_api_key:\n",
        "    raise ValueError(\"GROQ_API_KEY not found in .env file\")\n",
        "\n",
        "print(\"Groq API key loaded successfully!\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
        "\n",
        "print(\"OpenAI API key loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZkflG4RVo-8"
      },
      "source": [
        "#### Building Tools for the Research Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oY1RXAdwVrOT"
      },
      "outputs": [],
      "source": [
        "# 1. Simple ArXiv Search Tool\n",
        "def search_arxiv(query, max_results=5):\n",
        "    \"\"\"Search for papers on arXiv and return basic info.\"\"\"\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(query=query, max_results=max_results)\n",
        "\n",
        "    results = []\n",
        "    for paper in client.results(search):\n",
        "        results.append({\n",
        "            \"id\": paper.entry_id.split(\"/\")[-1],\n",
        "            \"title\": paper.title,\n",
        "            \"authors\": [author.name for author in paper.authors],\n",
        "            \"url\": paper.pdf_url\n",
        "        })\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G4e4Ip1uVt1S"
      },
      "outputs": [],
      "source": [
        "# 2. Simple Paper Downloader\n",
        "def download_paper(paper_id):\n",
        "    \"\"\"Download a paper from arXiv by ID and extract text.\"\"\"\n",
        "    search = arxiv.Search(id_list=[paper_id])\n",
        "    client = arxiv.Client()\n",
        "    paper = next(client.results(search))\n",
        "\n",
        "    response = requests.get(paper.pdf_url)\n",
        "    pdf_file = BytesIO(response.content)\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "\n",
        "    return text[:5000]  # Return first 5000 chars for simplicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t7oOCwDpVwZl"
      },
      "outputs": [],
      "source": [
        "# 3. Simple Summarizer \n",
        "# Using Groq to summarize the text. OpenAI has a similar model but we are using Groq to keep the costs down.\n",
        "\n",
        "def summarize_text(text, groq_api_key):\n",
        "    \"\"\"Summarize text using Groq.\"\"\"\n",
        "    model = ChatGroq(api_key=groq_api_key, model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
        "    prompt = f\"Summarize this academic paper: {text[:15000]}\"\n",
        "    return model.invoke(prompt).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4_T9YDXHV2FA"
      },
      "outputs": [],
      "source": [
        "# 4. Create the tool wrapper for our agent\n",
        "def create_tools(groq_api_key: str):\n",
        "    \"\"\"Create the tools for our agent.\"\"\"\n",
        "\n",
        "    tools = [\n",
        "        Tool(\n",
        "            name=\"search_arxiv\",\n",
        "            description=\"Search for academic papers on arXiv. Input is a search query.\",\n",
        "            func=lambda q: search_arxiv(q)\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"download_paper\",\n",
        "            description=\"Download a paper from arXiv and extract its text. Input is a paper ID.\",\n",
        "            func=lambda id: download_paper(id)\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"summarize_text\",\n",
        "            description=\"Summarize a piece of text. Input is the text to summarize.\",\n",
        "            func=lambda t: summarize_text(t, groq_api_key)\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oyNDKE2V3ZT"
      },
      "source": [
        "#### Putting it together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jtTO0dKNXbAj"
      },
      "outputs": [],
      "source": [
        "# First, create a prompt template using langchain\n",
        "# Definging the role of the agent and the input and output format\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant that can use tools to answer user questions about research papers.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])\n",
        "\n",
        "def setup_augmented_llm(groq_api_key):\n",
        "    # Define the tools\n",
        "    tools = create_tools(groq_api_key)\n",
        "\n",
        "    # Create the LLM\n",
        "    llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "\n",
        "    # Create the agent with tools\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "    # Create an agent executor\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    return agent_executor\n",
        "\n",
        "# 5. Run the augmented LLM, this is the main function that will be used to answer the user's question. \n",
        "def run_augmented_llm(query, groq_api_key):\n",
        "    agent_executor = setup_augmented_llm(groq_api_key)\n",
        "    result = agent_executor.invoke({\"input\": query})\n",
        "    return result[\"output\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufDX-MpdPr93"
      },
      "source": [
        "#### The Augmented LLM - Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pY8KEpaPwqR",
        "outputId": "d6f3e888-fff7-45bd-a9ed-14f956560561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `search_arxiv` with `latest developments in quantum computing`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'id': '0305472v2', 'title': 'Arbitrage risk induced by transaction costs', 'authors': ['E. W. Piotrowski', 'J. Sladkowski'], 'url': 'http://arxiv.org/pdf/cond-mat/0305472v2'}, {'id': '2302.00001v1', 'title': 'A Living Review of Quantum Computing for Plasma Physics', 'authors': ['√ìscar Amaro', 'Diogo Cruz'], 'url': 'http://arxiv.org/pdf/2302.00001v1'}, {'id': '2502.08925v1', 'title': 'Quantum Software Engineering and Potential of Quantum Computing in Software Engineering Research: A Review', 'authors': ['Ashis Kumar Mandal', 'Md Nadim', 'Chanchal K. Roy', 'Banani Roy', 'Kevin A. Schneider'], 'url': 'http://arxiv.org/pdf/2502.08925v1'}, {'id': '2407.16296v1', 'title': 'Quantum Computing for Climate Resilience and Sustainability Challenges', 'authors': ['Kin Tung Michael Ho', 'Kuan-Cheng Chen', 'Lily Lee', 'Felix Burt', 'Shang Yu', 'Po-Heng', 'Lee'], 'url': 'http://arxiv.org/pdf/2407.16296v1'}, {'id': '1912.00850v1', 'title': 'QFT with FDR', 'authors': ['Roberto Pittau'], 'url': 'http://arxiv.org/pdf/1912.00850v1'}]\u001b[0m\u001b[32;1m\u001b[1;3mRecent advances in quantum computing span several areas‚Äîfrom hardware and error correction to algorithms and interdisciplinary applications. Here are some key developments:\n",
            "\n",
            "1. Hardware improvements and scalability: Researchers are making progress on the physical realization of quantum bits (qubits). Platforms like superconducting circuits, trapped ions, and photonic systems are being refined to enhance coherence times, reduce error rates, and scale up the number of qubits. These efforts are critical in moving towards fault-tolerant quantum computers.\n",
            "\n",
            "2. Quantum error correction and fault tolerance: Traditional error correction techniques have been adapted and improved for the quantum realm. New error correction codes and methodologies are being developed to reliably overcome the decoherence and noise issues inherent in quantum systems. These techniques are essential for the reliable operation of quantum algorithms over extended periods.\n",
            "\n",
            "3. Algorithm development and software engineering: There is an increasing emphasis on quantum algorithms that can leverage quantum hardware effectively. Research in quantum software engineering is addressing challenges in designing, testing, and optimizing algorithms to solve specific problems in fields ranging from optimization to machine learning. For instance, recent reviews have focused on the potential of quantum computing in software engineering contexts, highlighting both theoretical and practical issues.\n",
            "\n",
            "4. Interdisciplinary and application-focused research: Quantum computing is expanding its reach into various scientific domains. Novel applications are emerging in fields like plasma physics, climate resilience, and sustainability. For example, a ‚ÄúLiving Review‚Äù article delves into how quantum computing can be applied in plasma physics, exploring practical computational challenges unique to the field.\n",
            "\n",
            "5. Quantum simulation: Alongside algorithm development, quantum simulation is a critical focus area‚Äîaiming to simulate complex quantum systems that are intractable for classical computers. These simulations have the potential to deepen our understanding of quantum materials, chemical systems, and even fundamental physics problems.\n",
            "\n",
            "In summary, the latest research is not only tackling the engineering and algorithmic challenges necessary for building larger, more reliable quantum computers but is also integrating quantum methods into various applied fields. These developments are paving the way for quantum computing to move from the laboratory into practical and industry-relevant applications.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Recent advances in quantum computing span several areas‚Äîfrom hardware and error correction to algorithms and interdisciplinary applications. Here are some key developments:\n",
            "\n",
            "1. Hardware improvements and scalability: Researchers are making progress on the physical realization of quantum bits (qubits). Platforms like superconducting circuits, trapped ions, and photonic systems are being refined to enhance coherence times, reduce error rates, and scale up the number of qubits. These efforts are critical in moving towards fault-tolerant quantum computers.\n",
            "\n",
            "2. Quantum error correction and fault tolerance: Traditional error correction techniques have been adapted and improved for the quantum realm. New error correction codes and methodologies are being developed to reliably overcome the decoherence and noise issues inherent in quantum systems. These techniques are essential for the reliable operation of quantum algorithms over extended periods.\n",
            "\n",
            "3. Algorithm development and software engineering: There is an increasing emphasis on quantum algorithms that can leverage quantum hardware effectively. Research in quantum software engineering is addressing challenges in designing, testing, and optimizing algorithms to solve specific problems in fields ranging from optimization to machine learning. For instance, recent reviews have focused on the potential of quantum computing in software engineering contexts, highlighting both theoretical and practical issues.\n",
            "\n",
            "4. Interdisciplinary and application-focused research: Quantum computing is expanding its reach into various scientific domains. Novel applications are emerging in fields like plasma physics, climate resilience, and sustainability. For example, a ‚ÄúLiving Review‚Äù article delves into how quantum computing can be applied in plasma physics, exploring practical computational challenges unique to the field.\n",
            "\n",
            "5. Quantum simulation: Alongside algorithm development, quantum simulation is a critical focus area‚Äîaiming to simulate complex quantum systems that are intractable for classical computers. These simulations have the potential to deepen our understanding of quantum materials, chemical systems, and even fundamental physics problems.\n",
            "\n",
            "In summary, the latest research is not only tackling the engineering and algorithmic challenges necessary for building larger, more reliable quantum computers but is also integrating quantum methods into various applied fields. These developments are paving the way for quantum computing to move from the laboratory into practical and industry-relevant applications.\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the latest developments in quantum computing?\"\n",
        "response = run_augmented_llm(query, groq_api_key)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZgHyK06i51-"
      },
      "source": [
        "## Prompt Chaining\n",
        "\n",
        "![Anthropic Prompt Chaining](https://drive.google.com/uc?export=view&id=1Rd_jjg1OaCxnvHFGszcwmaSALh1ZWVII)\n",
        "\n",
        "This workflow pattern chains output of one LLM Call / Tool to the other one.\n",
        "\n",
        "The research assistant is the orchestrator that coordinates the search and synthesis.\n",
        "search_llm and synthesis_llm are the two LLMs in this workflow. The output of search_llm is fed into the input of synthesis_llm.\n",
        "search_llm is used to convert the user's question into a search query using openai.\n",
        "synthesis_llm is used to synthesize the results of the search using groq."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgZcTuQapDSP"
      },
      "source": [
        "### Setup Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JTxnmYL1nkGj"
      },
      "outputs": [],
      "source": [
        "def setup_prompt_chain(groq_api_key, openai_api_key):\n",
        "    \"\"\"\n",
        "    Setup a chain of prompts where the output of one LLM call is fed into another.\n",
        "    This demonstrates the Prompt Chaining pattern.\n",
        "    \"\"\"\n",
        "    # Create the LLMs\n",
        "    search_llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "    synthesis_llm = ChatGroq(api_key=groq_api_key, model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
        "\n",
        "    # Create the tools\n",
        "    tools = create_tools(groq_api_key)\n",
        "\n",
        "    # STEP 1: Create the search agent prompt\n",
        "    search_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a research assistant that helps find relevant papers on arXiv.\n",
        "Your job is to convert a user's research question into the best possible search query for arXiv.\n",
        "Focus on extracting key technical terms and concepts.\n",
        "Respond ONLY with the search query, nothing else.\"\"\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "\n",
        "    # STEP 2: Create the paper selection agent prompt\n",
        "    selection_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a research assistant that helps select the most relevant paper from search results.\n",
        "Given a list of papers and the original research question, select the SINGLE most relevant paper ID.\n",
        "Respond ONLY with the paper ID, nothing else.\"\"\"),\n",
        "        (\"human\", \"Research question: {original_query}\\nSearch results: {search_results}\")\n",
        "    ])\n",
        "\n",
        "    # STEP 3: Create the analysis agent prompt\n",
        "    analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a research expert that analyzes academic papers.\n",
        "Given the text of a paper, identify the key methodologies, findings, and implications.\n",
        "Focus on how this paper addresses the user's original question.\"\"\"),\n",
        "        (\"human\", \"Original question: {original_query}\\nPaper text: {paper_text}\")\n",
        "    ])\n",
        "\n",
        "    # STEP 4: Create the final synthesis prompt\n",
        "    synthesis_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a helpful AI research assistant that synthesizes information into clear,\n",
        "concise responses. Create a comprehensive yet accessible answer to the user's question based on the\n",
        "technical analysis provided.\"\"\"),\n",
        "        (\"human\", \"Original question: {original_query}\\nTechnical analysis: {analysis}\")\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        \"search_llm\": search_llm,\n",
        "        \"synthesis_llm\": synthesis_llm,\n",
        "        \"tools\": tools,\n",
        "        \"prompts\": {\n",
        "            \"search\": search_prompt,\n",
        "            \"selection\": selection_prompt,\n",
        "            \"analysis\": analysis_prompt,\n",
        "            \"synthesis\": synthesis_prompt\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwMNfyscpHjT"
      },
      "source": [
        "### Setup Chaining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rZa050dLnnkY"
      },
      "outputs": [],
      "source": [
        "def run_prompt_chain(query, groq_api_key, openai_api_key):\n",
        "    \"\"\"\n",
        "    Execute the entire prompt chain to answer a research question.\n",
        "    This demonstrates how outputs from one LLM call are fed as inputs to the next.\n",
        "    \"\"\"\n",
        "    # Setup the chain components\n",
        "    chain = setup_prompt_chain(groq_api_key, openai_api_key)\n",
        "    search_llm = chain[\"search_llm\"]\n",
        "    synthesis_llm = chain[\"synthesis_llm\"]\n",
        "    tools = chain[\"tools\"]\n",
        "    prompts = chain[\"prompts\"]\n",
        "\n",
        "    print(\"üìù STARTING PROMPT CHAIN\")\n",
        "    print(f\"üìã Original query: {query}\")\n",
        "\n",
        "    # STEP 1: Convert question to search query\n",
        "    print(\"\\nüîç Step 1: Converting question to optimal search query...\")\n",
        "    search_query = search_llm.invoke(prompts[\"search\"].format(input=query)).content\n",
        "    print(f\"üîç Generated search query: {search_query}\")\n",
        "\n",
        "    # STEP 2: Search for papers\n",
        "    print(\"\\nüìö Step 2: Searching for papers on arXiv...\")\n",
        "    search_results = search_arxiv(search_query)\n",
        "    print(f\"üìö Found {len(search_results)} papers\")\n",
        "\n",
        "    # STEP 3: Select the most relevant paper\n",
        "    print(\"\\nüîé Step 3: Selecting the most relevant paper...\")\n",
        "    paper_id = search_llm.invoke(\n",
        "        prompts[\"selection\"].format(\n",
        "            original_query=query,\n",
        "            search_results=search_results\n",
        "        )\n",
        "    ).content\n",
        "    print(f\"üîé Selected paper ID: {paper_id}\")\n",
        "\n",
        "    # STEP 4: Download the selected paper\n",
        "    print(\"\\nüìÑ Step 4: Downloading the selected paper...\")\n",
        "    paper_text = download_paper(paper_id)\n",
        "    print(f\"üìÑ Downloaded {len(paper_text)} characters of paper text\")\n",
        "\n",
        "    # STEP 5: Analyze the paper\n",
        "    print(\"\\nüß™ Step 5: Analyzing the paper...\")\n",
        "    analysis = search_llm.invoke(\n",
        "        prompts[\"analysis\"].format(\n",
        "            original_query=query,\n",
        "            paper_text=paper_text\n",
        "        )\n",
        "    ).content\n",
        "    print(f\"üß™ Analysis complete\")\n",
        "\n",
        "    # STEP 6: Final synthesis\n",
        "    print(\"\\nüîÆ Step 6: Synthesizing final response...\")\n",
        "    final_response = synthesis_llm.invoke(\n",
        "        prompts[\"synthesis\"].format(\n",
        "            original_query=query,\n",
        "            analysis=analysis\n",
        "        )\n",
        "    ).content\n",
        "\n",
        "    print(\"\\n‚úÖ PROMPT CHAIN COMPLETE\")\n",
        "\n",
        "    return {\n",
        "        \"search_query\": search_query,\n",
        "        \"selected_paper\": next((p for p in search_results if p[\"id\"] == paper_id), None),\n",
        "        \"analysis\": analysis,\n",
        "        \"response\": final_response\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxsG3eqfnqPT",
        "outputId": "40d9f2eb-3242-4ebf-869b-57413840e7f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù STARTING PROMPT CHAIN\n",
            "üìã Original query: What are the latest breakthroughs in quantum error correction?\n",
            "\n",
            "üîç Step 1: Converting question to optimal search query...\n",
            "üîç Generated search query: quantum error correction recent breakthroughs quantum computing advanced coding techniques\n",
            "\n",
            "üìö Step 2: Searching for papers on arXiv...\n",
            "üìö Found 5 papers\n",
            "\n",
            "üîé Step 3: Selecting the most relevant paper...\n",
            "üîé Selected paper ID: 2412.21171v2\n",
            "\n",
            "üìÑ Step 4: Downloading the selected paper...\n",
            "üìÑ Downloaded 5000 characters of paper text\n",
            "\n",
            "üß™ Step 5: Analyzing the paper...\n",
            "üß™ Analysis complete\n",
            "\n",
            "üîÆ Step 6: Synthesizing final response...\n",
            "\n",
            "‚úÖ PROMPT CHAIN COMPLETE\n",
            "\n",
            "----- FINAL RESPONSE -----\n",
            "The latest breakthroughs in quantum error correction involve the development of quantum error-correcting codes that approach the theoretical hashing bound, a fundamental limit on quantum channel capacity, while maintaining computational efficiency. Researchers have made significant advancements by adapting classical low-density parity-check (LDPC) codes to create quantum LDPC codes. These codes have achieved near-bound performance, scalability, and reduced error floors, making them a promising solution for large-scale, fault-tolerant quantum computing.\n",
            "\n",
            "Key advancements include:\n",
            "\n",
            "1. **Construction of quantum LDPC codes from classical LDPC codes**: By leveraging the well-understood properties of classical codes, researchers have developed quantum error-correcting codes that can approach the hashing bound.\n",
            "2. **Approaching the hashing bound**: The developed quantum codes have demonstrated error correction performance that was previously unattainable with efficient decoding schemes.\n",
            "3. **Scalability and computational efficiency**: The technique maintains linear decoding complexity, making it viable for large-scale quantum computing systems that require millions of logical qubits.\n",
            "4. **Error-floor reduction**: Design choices, such as using affine permutation matrix (APM)-LDPC codes, have effectively reduced the error floor phenomenon, enabling extremely low error rates in the quantum regime.\n",
            "\n",
            "These breakthroughs have significant implications for the development of practical, large-scale quantum computers. By providing a pathway to manage fault tolerance over a vast number of physical qubits, researchers are closer to realizing quantum machines capable of handling real-world problems. The integration of error correction schemes with devices managing large numbers of qubits is a crucial step toward achieving fault-tolerant quantum computing.\n",
            "\n",
            "Overall, the latest breakthroughs in quantum error correction represent a significant advancement in both theoretical understanding and practical solutions, paving the way for the future scalability of quantum computing systems.\n",
            "\n",
            "--------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'search_query': 'quantum error correction recent breakthroughs quantum computing advanced coding techniques',\n",
              " 'selected_paper': {'id': '2412.21171v2',\n",
              "  'title': 'Quantum Error Correction near the Coding Theoretical Bound',\n",
              "  'authors': ['Daiki Komoto', 'Kenta Kasai'],\n",
              "  'url': 'http://arxiv.org/pdf/2412.21171v2'},\n",
              " 'analysis': 'Below is a concise analysis of the paper ‚ÄúQuantum Error Correction near the Coding Theoretical Bound‚Äù by Komoto and Kasai, with respect to the original question about the latest breakthroughs in quantum error correction:\\n\\n1. Key Methodologies:\\n\\u2003‚Ä¢ Construction from Classical LDPC Codes: The authors develop quantum error‚Äêcorrecting codes by adapting classical low-density parity-check (LDPC) codes‚Äîincluding protograph-based variants such as quasi-cyclic (QC-LDPC) and affine permutation matrix (APM)-LDPC codes. This cross-domain strategy leverages the well-understood properties of classical codes.\\n\\u2003‚Ä¢ Approaching Theoretical Bounds: The paper focuses on designing quantum codes whose performance nears the hashing bound, a fundamental lower limit on the quantum channel capacity. This involves aligning the performance of the quantum codes with the optimal benchmark known from classical error correction.\\n\\u2003‚Ä¢ Mitigating Error Floor: By addressing issues related to short cycles (which contribute to error floors) in LDPC matrices, the study uses APM-LDPC codes that can exceed known girth limits (compared to the upper bound of 12 for QC-LDPC matrices) to minimize error floor problems in high-fidelity regimes.\\n\\u2003‚Ä¢ Computational Efficiency: Emphasizing linear computational complexity in the number of physical qubits, the methodology tackles the scalability challenge for practical implementations that need millions of logical qubits.\\n\\n2. Key Findings:\\n\\u2003‚Ä¢ Near-Bound Performance: The paper demonstrates that quantum error-correcting codes constructed from classical LDPC codes can approach the hashing bound, thereby achieving error correction performance that was previously unattainable with efficient decoding schemes.\\n\\u2003‚Ä¢ Scalability: By maintaining linear decoding complexity, the technique provides a viable pathway for error correction in systems designed to support large-scale, fault-tolerant quantum computing.\\n\\u2003‚Ä¢ Error-Floor Reduction: The design choices (such as using APM-LDPC matrices with higher girth) effectively reduce the error floor phenomenon, potentially enabling extremely low error rates in the quantum regime.\\n\\n3. Implications and Relevance to the Original Question:\\n\\u2003‚Ä¢ Breakthrough in Code Efficiency: One of the latest breakthroughs highlighted in this work is the ability to combine classical coding theory with quantum error correction to produce codes that not only approach the theoretical limits (hashing bound) but also do so with an efficient (linear) decoding procedure.\\n\\u2003‚Ä¢ Pathway Toward Large-Scale Quantum Computation: The work directly addresses the challenge of scaling quantum computers. By showing how to manage fault tolerance over a vast number of physical qubits, the paper provides a promising route to realizing practical, large-scale quantum machines capable of handling real-world problems.\\n\\u2003‚Ä¢ Theoretical and Practical Integration: The study bridges a gap between theoretical error correction performance and practical, hardware-level implementations. It outlines how integrating error correction schemes with devices managing large numbers of qubits could bring us closer to fault-tolerant quantum computers.\\n\\nIn summary, the paper represents a significant breakthrough by introducing quantum LDPC codes that near the hashing bound while ensuring computational efficiency. This approach not only advances theoretical understanding in quantum error correction but also offers practical solutions essential for the future scalability of quantum computing systems, directly addressing the core of the original inquiry on the latest breakthroughs in this field.',\n",
              " 'response': 'The latest breakthroughs in quantum error correction involve the development of quantum error-correcting codes that approach the theoretical hashing bound, a fundamental limit on quantum channel capacity, while maintaining computational efficiency. Researchers have made significant advancements by adapting classical low-density parity-check (LDPC) codes to create quantum LDPC codes. These codes have achieved near-bound performance, scalability, and reduced error floors, making them a promising solution for large-scale, fault-tolerant quantum computing.\\n\\nKey advancements include:\\n\\n1. **Construction of quantum LDPC codes from classical LDPC codes**: By leveraging the well-understood properties of classical codes, researchers have developed quantum error-correcting codes that can approach the hashing bound.\\n2. **Approaching the hashing bound**: The developed quantum codes have demonstrated error correction performance that was previously unattainable with efficient decoding schemes.\\n3. **Scalability and computational efficiency**: The technique maintains linear decoding complexity, making it viable for large-scale quantum computing systems that require millions of logical qubits.\\n4. **Error-floor reduction**: Design choices, such as using affine permutation matrix (APM)-LDPC codes, have effectively reduced the error floor phenomenon, enabling extremely low error rates in the quantum regime.\\n\\nThese breakthroughs have significant implications for the development of practical, large-scale quantum computers. By providing a pathway to manage fault tolerance over a vast number of physical qubits, researchers are closer to realizing quantum machines capable of handling real-world problems. The integration of error correction schemes with devices managing large numbers of qubits is a crucial step toward achieving fault-tolerant quantum computing.\\n\\nOverall, the latest breakthroughs in quantum error correction represent a significant advancement in both theoretical understanding and practical solutions, paving the way for the future scalability of quantum computing systems.'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage\n",
        "def run_example():\n",
        "    # Load API keys from environment\n",
        "    # groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    # openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Run the prompt chain\n",
        "    query = \"What are the latest breakthroughs in quantum error correction?\"\n",
        "    result = run_prompt_chain(query, groq_api_key, openai_api_key)\n",
        "\n",
        "    # Output the final response\n",
        "    print(\"\\n----- FINAL RESPONSE -----\")\n",
        "    print(result[\"response\"])\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return result\n",
        "\n",
        "run_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbsouuK6q3LA"
      },
      "source": [
        "## Routing\n",
        "Routing is a pattern where the agent analyzes the query and directs it to the appropriate specialized pathway.\n",
        "\n",
        "![Anthropic Prompt Chaining](https://drive.google.com/uc?export=view&id=1e_Sq2alvr7rn10iwEKshtQ4E78435pCH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sDlEGLTJq98X"
      },
      "outputs": [],
      "source": [
        "class RouterAgent:\n",
        "    \"\"\"\n",
        "    A router that analyzes the query and directs it to the appropriate specialized pathway.\n",
        "    \"\"\"\n",
        "    def __init__(self, openai_api_key):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.router_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a query classifier that determines the type of research question.\n",
        "Analyze the user's research question and classify it into EXACTLY ONE of the following categories:\n",
        "1. \"latest_developments\" - Questions about recent advancements or state-of-the-art in a field\n",
        "2. \"deep_dive\" - Questions requiring in-depth analysis of specific techniques or methodologies\n",
        "3. \"comparison\" - Questions comparing different approaches, methods, or theories\n",
        "4. \"application\" - Questions about applying research to specific domains or problems\n",
        "\n",
        "Respond ONLY with the category name in lowercase, nothing else.\"\"\"),\n",
        "            (\"human\", \"{query}\")\n",
        "        ])\n",
        "\n",
        "    def route(self, query):\n",
        "        \"\"\"Determine which specialized pathway should handle this query.\"\"\"\n",
        "        response = self.llm.invoke(self.router_prompt.format(query=query))\n",
        "        query_type = response.content.strip().lower()\n",
        "\n",
        "        # Validate that the response is one of our expected categories\n",
        "        valid_types = [\"latest_developments\", \"deep_dive\", \"comparison\", \"application\"]\n",
        "        if query_type not in valid_types:\n",
        "            # Default to latest_developments if classification fails\n",
        "            query_type = \"latest_developments\"\n",
        "\n",
        "        return query_type\n",
        "\n",
        "class LatestDevelopmentsAgent:\n",
        "    \"\"\"Specialized agent for handling queries about recent developments.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in the latest developments and\n",
        "state-of-the-art research. Your goal is to find the most recent papers on a topic and\n",
        "summarize the cutting-edge advancements.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "class DeepDiveAgent:\n",
        "    \"\"\"Specialized agent for handling deep-dive queries.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in deep technical analysis.\n",
        "Your goal is to find and thoroughly analyze papers on specific techniques or methodologies.\n",
        "Focus on understanding the core principles, implementation details, and theoretical foundations.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "class ComparisonAgent:\n",
        "    \"\"\"Specialized agent for handling comparison queries.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in comparative analysis.\n",
        "Your goal is to find papers that compare different approaches, methods, or theories and\n",
        "analyze the strengths and weaknesses of each. Create a balanced assessment that highlights\n",
        "the key differences.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "class ApplicationAgent:\n",
        "    \"\"\"Specialized agent for handling application queries.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in practical applications.\n",
        "Your goal is to find papers that demonstrate how research is applied to real-world problems\n",
        "and domains. Focus on implementation details, case studies, and practical outcomes.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "def setup_routing_system(openai_api_key, groq_api_key):\n",
        "    \"\"\"Set up the complete routing system with specialized agents.\"\"\"\n",
        "    tools = create_tools(groq_api_key)\n",
        "\n",
        "    # Create the router and specialized agents\n",
        "    router = RouterAgent(openai_api_key)\n",
        "    agents = {\n",
        "        \"latest_developments\": LatestDevelopmentsAgent(openai_api_key, groq_api_key, tools),\n",
        "        \"deep_dive\": DeepDiveAgent(openai_api_key, groq_api_key, tools),\n",
        "        \"comparison\": ComparisonAgent(openai_api_key, groq_api_key, tools),\n",
        "        \"application\": ApplicationAgent(openai_api_key, groq_api_key, tools)\n",
        "    }\n",
        "\n",
        "    return router, agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RmiPXdlrYW8",
        "outputId": "01cd4ad5-98e0-4e62-e3d2-93675ef8b9b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üö¶ ROUTING SYSTEM INITIATED\n",
            "üìã Query: How do transformers handle attention mechanisms in deep learning?\n",
            "\n",
            "üîÄ Routing query...\n",
            "üîÄ Query classified as: deep_dive\n",
            "\n",
            "üî¨ Processing with specialized deep_dive agent...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTransformers revolutionized deep learning for sequence modeling by relying almost entirely on attention mechanisms‚Äîparticularly ‚Äúself-attention‚Äù‚Äîto capture dependencies across tokens without the need for recurrence or convolutions. Here‚Äôs a detailed technical breakdown of how they handle attention:\n",
            "\n",
            "1. Core idea: In a transformer, each token (word, image patch, etc.) is first embedded into a continuous vector space. The model then computes three distinct representations for each token: a Query (Q), a Key (K), and a Value (V). These are typically derived by learned linear transformations from the input embeddings.\n",
            "\n",
            "2. Scaled Dot-Product Attention: The attention mechanism computes an attention score between every pair of tokens via a dot product between the Query of one token and the Key of another. More formally, given a query Q and keys K, the raw attention score is computed as Q¬∑K·µÄ. To maintain stable gradients when the dimension of the keys (d‚Çñ) is large, the dot product is scaled by 1/‚àö(d‚Çñ). The softmax function is then applied to obtain a distribution over the tokens. Finally, these scores are used to compute a weighted sum of the Value vectors, effectively allowing each token to ‚Äúpay attention‚Äù to all others in the input.\n",
            "\n",
            "3. Multi-Head Attention: Instead of performing a single attention operation, transformers use multiple attention ‚Äúheads.‚Äù Each head has its own set of learned linear transformations for Q, K, and V. This diversification allows different heads to focus on different types of relationships or patterns in the data. After computing these multiple independent attention outputs, they are concatenated and projected back into the desired output space.\n",
            "\n",
            "4. Self-Attention vs. Encoder-Decoder Attention:\n",
            "   - Self-Attention: Within the encoder and decoder blocks, self-attention allows each position in the sequence to aggregate information from all positions. This is crucial for capturing long-range dependencies.\n",
            "   - Encoder-Decoder Attention: In the decoder, an additional attention mechanism is used to bring in information from the encoder. This mechanism essentially uses the decoder‚Äôs self-attention outputs as queries while the encoder‚Äôs output represents both keys and values.\n",
            "\n",
            "5. Positional Encodings: Since there is no recurrence or convolution to capture the order of sequences, transformers add positional encodings to the input embeddings. These encodings can be fixed (like sine and cosine functions of varying frequencies) or learned, ensuring that the model can capture positional relationships between tokens.\n",
            "\n",
            "6. Efficiency Benefits and Parallelization: By replacing sequential recurrence with fully parallelizable attention operations, transformers can process sequence elements simultaneously, greatly enhancing training speed and enabling the handling of long sequences more efficiently.\n",
            "\n",
            "The seminal paper ‚ÄúAttention is All You Need‚Äù (Vaswani et al., 2017) laid the theoretical and architectural foundation for these techniques and has been extensively analyzed in subsequent research. Transformers continue to be extended, with variants that modify the attention mechanism (such as sparse or adaptive attention) to further improve interpretability and efficiency.\n",
            "\n",
            "In summary, transformers leverage the attention mechanism to dynamically weight the importance of different parts of the input sequence. This allows the model to capture complex, long-distance dependencies and to process data in a parallelizable manner, significantly enhancing both performance and scalability in deep learning applications.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "‚úÖ ROUTING COMPLETE\n",
            "\n",
            "----- FINAL RESPONSE -----\n",
            "Transformers revolutionized deep learning for sequence modeling by relying almost entirely on attention mechanisms‚Äîparticularly ‚Äúself-attention‚Äù‚Äîto capture dependencies across tokens without the need for recurrence or convolutions. Here‚Äôs a detailed technical breakdown of how they handle attention:\n",
            "\n",
            "1. Core idea: In a transformer, each token (word, image patch, etc.) is first embedded into a continuous vector space. The model then computes three distinct representations for each token: a Query (Q), a Key (K), and a Value (V). These are typically derived by learned linear transformations from the input embeddings.\n",
            "\n",
            "2. Scaled Dot-Product Attention: The attention mechanism computes an attention score between every pair of tokens via a dot product between the Query of one token and the Key of another. More formally, given a query Q and keys K, the raw attention score is computed as Q¬∑K·µÄ. To maintain stable gradients when the dimension of the keys (d‚Çñ) is large, the dot product is scaled by 1/‚àö(d‚Çñ). The softmax function is then applied to obtain a distribution over the tokens. Finally, these scores are used to compute a weighted sum of the Value vectors, effectively allowing each token to ‚Äúpay attention‚Äù to all others in the input.\n",
            "\n",
            "3. Multi-Head Attention: Instead of performing a single attention operation, transformers use multiple attention ‚Äúheads.‚Äù Each head has its own set of learned linear transformations for Q, K, and V. This diversification allows different heads to focus on different types of relationships or patterns in the data. After computing these multiple independent attention outputs, they are concatenated and projected back into the desired output space.\n",
            "\n",
            "4. Self-Attention vs. Encoder-Decoder Attention:\n",
            "   - Self-Attention: Within the encoder and decoder blocks, self-attention allows each position in the sequence to aggregate information from all positions. This is crucial for capturing long-range dependencies.\n",
            "   - Encoder-Decoder Attention: In the decoder, an additional attention mechanism is used to bring in information from the encoder. This mechanism essentially uses the decoder‚Äôs self-attention outputs as queries while the encoder‚Äôs output represents both keys and values.\n",
            "\n",
            "5. Positional Encodings: Since there is no recurrence or convolution to capture the order of sequences, transformers add positional encodings to the input embeddings. These encodings can be fixed (like sine and cosine functions of varying frequencies) or learned, ensuring that the model can capture positional relationships between tokens.\n",
            "\n",
            "6. Efficiency Benefits and Parallelization: By replacing sequential recurrence with fully parallelizable attention operations, transformers can process sequence elements simultaneously, greatly enhancing training speed and enabling the handling of long sequences more efficiently.\n",
            "\n",
            "The seminal paper ‚ÄúAttention is All You Need‚Äù (Vaswani et al., 2017) laid the theoretical and architectural foundation for these techniques and has been extensively analyzed in subsequent research. Transformers continue to be extended, with variants that modify the attention mechanism (such as sparse or adaptive attention) to further improve interpretability and efficiency.\n",
            "\n",
            "In summary, transformers leverage the attention mechanism to dynamically weight the importance of different parts of the input sequence. This allows the model to capture complex, long-distance dependencies and to process data in a parallelizable manner, significantly enhancing both performance and scalability in deep learning applications.\n",
            "\n",
            "--------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query_type': 'deep_dive',\n",
              " 'response': 'Transformers revolutionized deep learning for sequence modeling by relying almost entirely on attention mechanisms‚Äîparticularly ‚Äúself-attention‚Äù‚Äîto capture dependencies across tokens without the need for recurrence or convolutions. Here‚Äôs a detailed technical breakdown of how they handle attention:\\n\\n1. Core idea: In a transformer, each token (word, image patch, etc.) is first embedded into a continuous vector space. The model then computes three distinct representations for each token: a Query (Q), a Key (K), and a Value (V). These are typically derived by learned linear transformations from the input embeddings.\\n\\n2. Scaled Dot-Product Attention: The attention mechanism computes an attention score between every pair of tokens via a dot product between the Query of one token and the Key of another. More formally, given a query Q and keys K, the raw attention score is computed as Q¬∑K·µÄ. To maintain stable gradients when the dimension of the keys (d‚Çñ) is large, the dot product is scaled by 1/‚àö(d‚Çñ). The softmax function is then applied to obtain a distribution over the tokens. Finally, these scores are used to compute a weighted sum of the Value vectors, effectively allowing each token to ‚Äúpay attention‚Äù to all others in the input.\\n\\n3. Multi-Head Attention: Instead of performing a single attention operation, transformers use multiple attention ‚Äúheads.‚Äù Each head has its own set of learned linear transformations for Q, K, and V. This diversification allows different heads to focus on different types of relationships or patterns in the data. After computing these multiple independent attention outputs, they are concatenated and projected back into the desired output space.\\n\\n4. Self-Attention vs. Encoder-Decoder Attention:\\n   - Self-Attention: Within the encoder and decoder blocks, self-attention allows each position in the sequence to aggregate information from all positions. This is crucial for capturing long-range dependencies.\\n   - Encoder-Decoder Attention: In the decoder, an additional attention mechanism is used to bring in information from the encoder. This mechanism essentially uses the decoder‚Äôs self-attention outputs as queries while the encoder‚Äôs output represents both keys and values.\\n\\n5. Positional Encodings: Since there is no recurrence or convolution to capture the order of sequences, transformers add positional encodings to the input embeddings. These encodings can be fixed (like sine and cosine functions of varying frequencies) or learned, ensuring that the model can capture positional relationships between tokens.\\n\\n6. Efficiency Benefits and Parallelization: By replacing sequential recurrence with fully parallelizable attention operations, transformers can process sequence elements simultaneously, greatly enhancing training speed and enabling the handling of long sequences more efficiently.\\n\\nThe seminal paper ‚ÄúAttention is All You Need‚Äù (Vaswani et al., 2017) laid the theoretical and architectural foundation for these techniques and has been extensively analyzed in subsequent research. Transformers continue to be extended, with variants that modify the attention mechanism (such as sparse or adaptive attention) to further improve interpretability and efficiency.\\n\\nIn summary, transformers leverage the attention mechanism to dynamically weight the importance of different parts of the input sequence. This allows the model to capture complex, long-distance dependencies and to process data in a parallelizable manner, significantly enhancing both performance and scalability in deep learning applications.'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def run_routing_system(query, openai_api_key, groq_api_key):\n",
        "    \"\"\"Run the routing system end-to-end.\"\"\"\n",
        "    router, agents = setup_routing_system(openai_api_key, groq_api_key)\n",
        "\n",
        "    print(\"üö¶ ROUTING SYSTEM INITIATED\")\n",
        "    print(f\"üìã Query: {query}\")\n",
        "\n",
        "    # Step 1: Route the query\n",
        "    print(\"\\nüîÄ Routing query...\")\n",
        "    query_type = router.route(query)\n",
        "    print(f\"üîÄ Query classified as: {query_type}\")\n",
        "\n",
        "    # Step 2: Process with the appropriate agent\n",
        "    print(f\"\\nüî¨ Processing with specialized {query_type} agent...\")\n",
        "    response = agents[query_type].process(query)\n",
        "\n",
        "    print(\"\\n‚úÖ ROUTING COMPLETE\")\n",
        "\n",
        "    return {\n",
        "        \"query_type\": query_type,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "def run_example():\n",
        "    # Load API keys from environment\n",
        "    # groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    # openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Example queries for different types\n",
        "    example_queries = {\n",
        "        \"latest_developments\": \"What are the latest advancements in quantum computing?\",\n",
        "        \"deep_dive\": \"How do transformers handle attention mechanisms in deep learning?\",\n",
        "        \"comparison\": \"Compare LSTM and transformer approaches for sequence modeling\",\n",
        "        \"application\": \"How is reinforcement learning applied to robotics?\"\n",
        "    }\n",
        "\n",
        "    # Choose one query to run\n",
        "    query = example_queries[\"deep_dive\"]\n",
        "\n",
        "    # Run the routing system\n",
        "    result = run_routing_system(query, openai_api_key, groq_api_key)\n",
        "\n",
        "    # Output the final response\n",
        "    print(\"\\n----- FINAL RESPONSE -----\")\n",
        "    print(result[\"response\"])\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return result\n",
        "\n",
        "run_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIP5opPY0pTM"
      },
      "source": [
        "## The Agent\n",
        "\n",
        "![Anthropic Prompt Chaining](https://drive.google.com/uc?export=view&id=1_ERsFpGuxGUq_lkfuoW0O5dc9nrSXoSy)\n",
        "\n",
        "Most flexible but difficult to get it right pattern, where you are relying on LLM (usually more than one) to provide a reasoned plan of action, observe and evaluate the results of the actions from environment, reason and decide on next steps including stopping the computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7BYczEQ1Z5n"
      },
      "source": [
        "LLM as Orchestrator & Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bHOer1VU00a8"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "class AgentAction(Enum):\n",
        "    \"\"\"Enum for different possible agent actions.\"\"\"\n",
        "    USE_TOOL = \"use_tool\"\n",
        "    ROUTE = \"route\"\n",
        "    FINAL_ANSWER = \"final_answer\"\n",
        "\n",
        "class ResearchPath(Enum):\n",
        "    \"\"\"Enum for different research paths.\"\"\"\n",
        "    OVERVIEW = \"overview\"\n",
        "    DEEP_DIVE = \"deep_dive\"\n",
        "    COMPARISON = \"comparison\"\n",
        "    TECHNICAL = \"technical\"\n",
        "\n",
        "class AgentState:\n",
        "    \"\"\"State container for the agent's reasoning process.\"\"\"\n",
        "    def __init__(self, query: str):\n",
        "        self.query = query\n",
        "        self.current_path: Optional[ResearchPath] = None\n",
        "        self.tool_results: List[Dict[str, Any]] = []\n",
        "        self.working_memory: List[str] = []\n",
        "        self.final_answer: Optional[str] = None\n",
        "        self.conversation_history: List[Dict[str, Any]] = []\n",
        "\n",
        "    def add_tool_result(self, tool_name: str, tool_input: str, tool_output: Any) -> None:\n",
        "        \"\"\"Add the result of a tool call to the state.\"\"\"\n",
        "        self.tool_results.append({\n",
        "            \"tool_name\": tool_name,\n",
        "            \"tool_input\": tool_input,\n",
        "            \"tool_output\": tool_output\n",
        "        })\n",
        "\n",
        "    def add_to_memory(self, thought: str) -> None:\n",
        "        \"\"\"Add a thought to working memory.\"\"\"\n",
        "        self.working_memory.append(thought)\n",
        "\n",
        "    def set_path(self, path: ResearchPath) -> None:\n",
        "        \"\"\"Set the current research path.\"\"\"\n",
        "        self.current_path = path\n",
        "\n",
        "    def set_final_answer(self, answer: str) -> None:\n",
        "        \"\"\"Set the final answer.\"\"\"\n",
        "        self.final_answer = answer\n",
        "\n",
        "    def add_message(self, role: str, content: str) -> None:\n",
        "        \"\"\"Add a message to the conversation history.\"\"\"\n",
        "        self.conversation_history.append({\n",
        "            \"role\": role,\n",
        "            \"content\": content\n",
        "        })\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert the state to a dictionary for serialization.\"\"\"\n",
        "        return {\n",
        "            \"query\": self.query,\n",
        "            \"current_path\": self.current_path.value if self.current_path else None,\n",
        "            \"tool_results\": self.tool_results,\n",
        "            \"working_memory\": self.working_memory,\n",
        "            \"final_answer\": self.final_answer,\n",
        "        }\n",
        "\n",
        "class ResearchAgent:\n",
        "    \"\"\"An agent that can dynamically make decisions about research queries.\"\"\"\n",
        "\n",
        "    def __init__(self, openai_api_key: str, groq_api_key: str):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\", temperature=0)\n",
        "        self.groq_api_key = groq_api_key\n",
        "        self.tools = create_tools(groq_api_key)\n",
        "\n",
        "        # Create the decision prompt template\n",
        "        self.decision_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant agent capable of dynamically making decisions about how to answer research questions.\n",
        "You must explicitly reason through THREE types of decisions:\n",
        "\n",
        "1. ROUTING - Determine which research path would best address the query:\n",
        "   - OVERVIEW: For broad, introductory questions needing a general summary\n",
        "   - DEEP_DIVE: For questions requiring in-depth focus on a specific topic\n",
        "   - COMPARISON: For questions comparing multiple approaches or methods\n",
        "   - TECHNICAL: For questions requiring detailed technical explanations\n",
        "\n",
        "2. TOOL SELECTION - Determine which tool to use next from:\n",
        "   - search_arxiv: Search for academic papers (input: search query)\n",
        "   - download_paper: Get the text of a specific paper (input: paper ID)\n",
        "   - summarize_text: Summarize a piece of text (input: text to summarize)\n",
        "\n",
        "3. FINAL ANSWER - Determine if you have enough information to provide a final answer\n",
        "\n",
        "For each decision, you MUST use the following process:\n",
        "1. Think through the options step-by-step\n",
        "2. Consider the query and current state\n",
        "3. Make an explicit choice\n",
        "4. Explain your reasoning\n",
        "\n",
        "Your response MUST be in this JSON format:\n",
        "{\n",
        "  \"thought\": \"Your detailed reasoning about the current state and what to do next\",\n",
        "  \"action\": \"ONE OF: use_tool, route, final_answer\",\n",
        "  \"action_input\": {\n",
        "    // For use_tool: \"tool_name\" and \"tool_input\"\n",
        "    // For route: \"path\"\n",
        "    // For final_answer: \"answer\"\n",
        "  }\n",
        "}\n",
        "\n",
        "You MUST ensure that your response is valid JSON with these exact fields.\"\"\"),\n",
        "            (\"human\", \"\"\"\n",
        "QUERY: {query}\n",
        "\n",
        "CURRENT PATH: {current_path}\n",
        "\n",
        "TOOL RESULTS:\n",
        "{tool_results}\n",
        "\n",
        "WORKING MEMORY:\n",
        "{working_memory}\n",
        "\n",
        "Based on the above, what should I do next? Think through whether I should choose a research path, use a tool, or provide a final answer.\n",
        "\"\"\")\n",
        "        ])\n",
        "\n",
        "    def _format_tool_results(self, tool_results: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Format tool results for the prompt.\"\"\"\n",
        "        if not tool_results:\n",
        "            return \"No tools have been used yet.\"\n",
        "\n",
        "        result_strings = []\n",
        "        for i, result in enumerate(tool_results):\n",
        "            result_str = f\"Tool {i+1}: {result['tool_name']}\\n\"\n",
        "            result_str += f\"Input: {result['tool_input']}\\n\"\n",
        "\n",
        "            # Format the output depending on the tool\n",
        "            if result['tool_name'] == \"search_arxiv\":\n",
        "                papers = result['tool_output']\n",
        "                result_str += \"Output (Papers):\\n\"\n",
        "                for j, paper in enumerate(papers):\n",
        "                    result_str += f\"  Paper {j+1}: {paper['title']}\\n\"\n",
        "                    result_str += f\"  ID: {paper['id']}\\n\"\n",
        "                    result_str += f\"  Authors: {', '.join(paper['authors'])}\\n\"\n",
        "                    result_str += f\"  URL: {paper['url']}\\n\\n\"\n",
        "            elif result['tool_name'] == \"download_paper\":\n",
        "                # Truncate long paper texts\n",
        "                paper_text = result['tool_output']\n",
        "                if len(paper_text) > 500:\n",
        "                    paper_text = paper_text[:500] + \"... [text truncated]\"\n",
        "                result_str += f\"Output (Paper Text):\\n{paper_text}\\n\\n\"\n",
        "            elif result['tool_name'] == \"summarize_text\":\n",
        "                result_str += f\"Output (Summary):\\n{result['tool_output']}\\n\\n\"\n",
        "\n",
        "            result_strings.append(result_str)\n",
        "\n",
        "        return \"\\n\".join(result_strings)\n",
        "\n",
        "    def _format_working_memory(self, working_memory: List[str]) -> str:\n",
        "        \"\"\"Format working memory for the prompt.\"\"\"\n",
        "        if not working_memory:\n",
        "            return \"No thoughts recorded yet.\"\n",
        "\n",
        "        memory_strings = []\n",
        "        for i, thought in enumerate(working_memory):\n",
        "            memory_strings.append(f\"Thought {i+1}: {thought}\")\n",
        "\n",
        "        return \"\\n\".join(memory_strings)\n",
        "\n",
        "    def _get_next_action(self, state: AgentState) -> Dict[str, Any]:\n",
        "        \"\"\"Get the next action from the LLM.\"\"\"\n",
        "        # Format the state for the prompt\n",
        "        prompt_args = {\n",
        "            \"query\": state.query,\n",
        "            \"current_path\": state.current_path.value if state.current_path else \"Not selected yet\",\n",
        "            \"tool_results\": self._format_tool_results(state.tool_results),\n",
        "            \"working_memory\": self._format_working_memory(state.working_memory)\n",
        "        }\n",
        "\n",
        "        # Get the LLM's decision\n",
        "        response = self.llm.invoke(self.decision_prompt.format(**prompt_args))\n",
        "\n",
        "        # Parse the JSON response\n",
        "        try:\n",
        "            # Extract JSON from the response (handling case where there might be extra text)\n",
        "            match = re.search(r'\\{.*\\}', response.content, re.DOTALL)\n",
        "            if match:\n",
        "                json_str = match.group(0)\n",
        "                action_data = json.loads(json_str)\n",
        "                return action_data\n",
        "            else:\n",
        "                # Fallback if no JSON found\n",
        "                return {\n",
        "                    \"thought\": \"Failed to parse response\",\n",
        "                    \"action\": \"final_answer\",\n",
        "                    \"action_input\": {\n",
        "                        \"answer\": \"I encountered an error in my reasoning process. Let me try a different approach: \" + response.content\n",
        "                    }\n",
        "                }\n",
        "        except json.JSONDecodeError:\n",
        "            # Handle invalid JSON\n",
        "            return {\n",
        "                \"thought\": \"Failed to parse JSON response\",\n",
        "                \"action\": \"final_answer\",\n",
        "                \"action_input\": {\n",
        "                    \"answer\": \"I encountered an error in my reasoning process. Let me try a different approach: \" + response.content\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def _execute_tool(self, tool_name: str, tool_input: str) -> Any:\n",
        "        \"\"\"Execute a tool and return the result.\"\"\"\n",
        "        # Find the tool\n",
        "        tool = next((t for t in self.tools if t.name == tool_name), None)\n",
        "        if not tool:\n",
        "            return f\"Error: Tool '{tool_name}' not found.\"\n",
        "\n",
        "        # Execute the tool\n",
        "        try:\n",
        "            result = tool.func(tool_input)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return f\"Error executing tool: {str(e)}\"\n",
        "\n",
        "    def _route_query(self, path_name: str) -> ResearchPath:\n",
        "        \"\"\"Route the query to a research path.\"\"\"\n",
        "        try:\n",
        "            return ResearchPath(path_name)\n",
        "        except ValueError:\n",
        "            # Default to overview if invalid path\n",
        "            return ResearchPath.OVERVIEW\n",
        "\n",
        "    def process_query(self, query: str, max_steps: int = 10) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a research query using the agent's reasoning loop.\n",
        "\n",
        "        Args:\n",
        "            query: The research query to process\n",
        "            max_steps: Maximum number of reasoning steps to perform\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the final answer and the agent's state\n",
        "        \"\"\"\n",
        "        # Initialize the agent state\n",
        "        state = AgentState(query)\n",
        "\n",
        "        print(\"ü§ñ RESEARCH AGENT INITIATED\")\n",
        "        print(f\"üìã Query: {query}\")\n",
        "\n",
        "        # Main reasoning loop\n",
        "        for step in range(max_steps):\n",
        "            print(f\"\\nüîÑ Step {step+1}: Reasoning...\")\n",
        "\n",
        "            # Get the next action\n",
        "            action_data = self._get_next_action(state)\n",
        "\n",
        "            # Add the thought to working memory\n",
        "            thought = action_data.get(\"thought\", \"No reasoning provided\")\n",
        "            state.add_to_memory(thought)\n",
        "            print(f\"üí≠ Thought: {thought}\")\n",
        "\n",
        "            # Parse the action\n",
        "            action = action_data.get(\"action\")\n",
        "            action_input = action_data.get(\"action_input\", {})\n",
        "\n",
        "            if action == AgentAction.USE_TOOL.value:\n",
        "                # Execute the tool\n",
        "                tool_name = action_input.get(\"tool_name\")\n",
        "                tool_input = action_input.get(\"tool_input\")\n",
        "\n",
        "                print(f\"üîß Using Tool: {tool_name}\")\n",
        "                print(f\"üì• Tool Input: {tool_input}\")\n",
        "\n",
        "                tool_output = self._execute_tool(tool_name, tool_input)\n",
        "                state.add_tool_result(tool_name, tool_input, tool_output)\n",
        "\n",
        "                print(f\"üì§ Tool Output: {type(tool_output)} with {len(str(tool_output))} chars\")\n",
        "\n",
        "            elif action == AgentAction.ROUTE.value:\n",
        "                # Set the research path\n",
        "                path_name = action_input.get(\"path\")\n",
        "                path = self._route_query(path_name)\n",
        "                state.set_path(path)\n",
        "\n",
        "                print(f\"üîÄ Routing to Path: {path.value}\")\n",
        "\n",
        "            elif action == AgentAction.FINAL_ANSWER.value:\n",
        "                # Set the final answer\n",
        "                answer = action_input.get(\"answer\")\n",
        "                state.set_final_answer(answer)\n",
        "\n",
        "                print(f\"‚úÖ Final Answer Ready\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                # Handle unknown action\n",
        "                print(f\"‚ö†Ô∏è Unknown Action: {action}\")\n",
        "                state.set_final_answer(f\"I encountered an error in my reasoning process. Unknown action: {action}\")\n",
        "                break\n",
        "        else:\n",
        "            # Handle max steps reached\n",
        "            state.set_final_answer(\"I've reached the maximum number of reasoning steps without finding a complete answer. Here's what I've learned so far: \" +\n",
        "                                 \"\\n\\n\".join(state.working_memory[-3:]))\n",
        "            print(\"‚ö†Ô∏è Maximum steps reached without final answer\")\n",
        "\n",
        "        print(\"\\n‚ú® REASONING COMPLETE\")\n",
        "\n",
        "        return {\n",
        "            \"final_answer\": state.final_answer,\n",
        "            \"state\": state.to_dict()\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "vnaJ5DyJ2VPm",
        "outputId": "0d0bb0f0-db01-48b3-b3bd-8f6f6239f3c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ RESEARCH AGENT INITIATED\n",
            "üìã Query: What are the differences between transformers and LSTMs for NLP tasks?\n",
            "\n",
            "üîÑ Step 1: Reasoning...\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'\\n  \"thought\"'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mrun_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mrun_example\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat are the differences between transformers and LSTMs for NLP tasks?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Process the query\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Output the final answer\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m----- FINAL ANSWER -----\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 249\u001b[39m, in \u001b[36mResearchAgent.process_query\u001b[39m\u001b[34m(self, query, max_steps)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîÑ Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Reasoning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Get the next action\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m action_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_next_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Add the thought to working memory\u001b[39;00m\n\u001b[32m    252\u001b[39m thought = action_data.get(\u001b[33m\"\u001b[39m\u001b[33mthought\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNo reasoning provided\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 176\u001b[39m, in \u001b[36mResearchAgent._get_next_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    168\u001b[39m prompt_args = {\n\u001b[32m    169\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: state.query,\n\u001b[32m    170\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcurrent_path\u001b[39m\u001b[33m\"\u001b[39m: state.current_path.value \u001b[38;5;28;01mif\u001b[39;00m state.current_path \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNot selected yet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    171\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtool_results\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_tool_results(state.tool_results),\n\u001b[32m    172\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mworking_memory\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_working_memory(state.working_memory)\n\u001b[32m    173\u001b[39m }\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Get the LLM's decision\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m response = \u001b[38;5;28mself\u001b[39m.llm.invoke(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecision_prompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprompt_args\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Parse the JSON response\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# Extract JSON from the response (handling case where there might be extra text)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/.venv/lib/python3.11/site-packages/langchain_core/prompts/chat.py:702\u001b[39m, in \u001b[36mBaseChatPromptTemplate.format\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    693\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format the chat template into a string.\u001b[39;00m\n\u001b[32m    694\u001b[39m \n\u001b[32m    695\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    700\u001b[39m \u001b[33;03m        formatted string.\u001b[39;00m\n\u001b[32m    701\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.to_string()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/.venv/lib/python3.11/site-packages/langchain_core/prompts/chat.py:725\u001b[39m, in \u001b[36mBaseChatPromptTemplate.format_prompt\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> PromptValue:\n\u001b[32m    717\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[32m    718\u001b[39m \n\u001b[32m    719\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    723\u001b[39m \u001b[33;03m        PromptValue.\u001b[39;00m\n\u001b[32m    724\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m     messages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages=messages)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/.venv/lib/python3.11/site-packages/langchain_core/prompts/chat.py:1186\u001b[39m, in \u001b[36mChatPromptTemplate.format_messages\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1182\u001b[39m     result.extend([message_template])\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   1184\u001b[39m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[32m   1185\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1186\u001b[39m     message = \u001b[43mmessage_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1187\u001b[39m     result.extend(message)\n\u001b[32m   1188\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/.venv/lib/python3.11/site-packages/langchain_core/prompts/chat.py:558\u001b[39m, in \u001b[36m_StringImageMessagePromptTemplate.format_messages\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[BaseMessage]:\n\u001b[32m    550\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format messages from kwargs.\u001b[39;00m\n\u001b[32m    551\u001b[39m \n\u001b[32m    552\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    556\u001b[39m \u001b[33;03m        List of BaseMessages.\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/.venv/lib/python3.11/site-packages/langchain_core/prompts/chat.py:591\u001b[39m, in \u001b[36m_StringImageMessagePromptTemplate.format\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Format the prompt template.\u001b[39;00m\n\u001b[32m    583\u001b[39m \n\u001b[32m    584\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    588\u001b[39m \u001b[33;03m    Formatted message.\u001b[39;00m\n\u001b[32m    589\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.prompt, StringPromptTemplate):\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m     text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._msg_class(\n\u001b[32m    593\u001b[39m         content=text, additional_kwargs=\u001b[38;5;28mself\u001b[39m.additional_kwargs\n\u001b[32m    594\u001b[39m     )\n\u001b[32m    595\u001b[39m content: \u001b[38;5;28mlist\u001b[39m = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/.venv/lib/python3.11/site-packages/langchain_core/prompts/prompt.py:186\u001b[39m, in \u001b[36mPromptTemplate.format\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \u001b[33;03m    A formatted string.\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    185\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m._merge_partial_and_user_variables(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULT_FORMATTER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/string.py:190\u001b[39m, in \u001b[36mFormatter.format\u001b[39m\u001b[34m(self, format_string, *args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, /, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/.venv/lib/python3.11/site-packages/langchain_core/utils/formatting.py:33\u001b[39m, in \u001b[36mStrictFormatter.vformat\u001b[39m\u001b[34m(self, format_string, args, kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m     msg = (\n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo arguments should be provided, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33meverything should be passed as keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m     )\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/string.py:194\u001b[39m, in \u001b[36mFormatter.vformat\u001b[39m\u001b[34m(self, format_string, args, kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, args, kwargs):\n\u001b[32m    193\u001b[39m     used_args = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     result, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_unused_args(used_args, args, kwargs)\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/string.py:234\u001b[39m, in \u001b[36mFormatter._vformat\u001b[39m\u001b[34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[39m\n\u001b[32m    230\u001b[39m     auto_arg_index = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m obj, arg_used = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m used_args.add(arg_used)\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# do any conversion on the resulting object\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/string.py:299\u001b[39m, in \u001b[36mFormatter.get_field\u001b[39m\u001b[34m(self, field_name, args, kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, args, kwargs):\n\u001b[32m    297\u001b[39m     first, rest = _string.formatter_field_name_split(field_name)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m     \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[32m    302\u001b[39m     \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m is_attr, i \u001b[38;5;129;01min\u001b[39;00m rest:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/string.py:256\u001b[39m, in \u001b[36mFormatter.get_value\u001b[39m\u001b[34m(self, key, args, kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m args[key]\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[31mKeyError\u001b[39m: '\\n  \"thought\"'"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "def run_example():\n",
        "    # Load API keys from environment\n",
        "    # groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    # openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Create the agent\n",
        "    agent = ResearchAgent(openai_api_key, groq_api_key)\n",
        "\n",
        "    # Example query\n",
        "    query = \"What are the differences between transformers and LSTMs for NLP tasks?\"\n",
        "\n",
        "    # Process the query\n",
        "    result = agent.process_query(query)\n",
        "\n",
        "    # Output the final answer\n",
        "    print(\"\\n----- FINAL ANSWER -----\")\n",
        "    print(result[\"final_answer\"])\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return result\n",
        "\n",
        "run_example()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vbsouuK6q3LA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
