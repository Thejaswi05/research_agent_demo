{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thejaswi05/research_agent_demo/blob/main/ODSC_AI_Research_Assistant_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9141TGmwseO"
      },
      "source": [
        "# **Problem**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For every scientist, academic, grad student, and engineer - literature review is a daunting task. We only wish we could automate getting insights and simplify work of finding papers and summarizing them. With advent of AI that is a distinct possibility. In this demo we attempt to build a Research Assistant  agent for this.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-12_LF3yL9c"
      },
      "source": [
        "#### **DISCLAIMER:**\n",
        "THIS IS NOT A NOVEL IDEA / PROBLEM, WE ARE USING THIS AS AN EXAMPLE TO EXPLAIN CONCEPTS OF BUILDING AGENTS. AGENT BUILT IS NOT MEANT FOR PRODUCTION USE. USE AT YOUR OWN RISK.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_b39R9Z6JYG"
      },
      "source": [
        "# **Building The Research Agent**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **LAYOUT:**\n",
        "In this notebook we will be working towards building an agent in a step by step manner. Anthropic released a blog post about build AI Agents and discussed some very important concepts - [Building Effective Agents by Anthropic]( https://www.anthropic.com/engineering/building-effective-agents). We will walk through some critical patterns that help us to build the agent.\n",
        "\n",
        "\n",
        "**Optional:** At the end of the notebook, there is an optional section that shows a succinct version of the agent using the LangGraph constructs to simplify the more education implementation. This would be useful for folks who want to see a succinct implementation.\n",
        "\n",
        "Following are the patterns we will explore:\n",
        "1. The Agumented Model\n",
        "2. Prompt Chaining\n",
        "3. Routing\n",
        "4. Parallelization - **Not Implemented**\n",
        "5. Orchestrator-workers - **Not Implemented**\n",
        "6. Evaluator-optimizer - **Not Implemented**\n",
        "7. The Agent\n",
        "\n",
        "**Image Credits:** We are borrowing some of the images from the [Building Effective Agents by Anthropic]( https://www.anthropic.com/engineering/building-effective-agents) blog. Copyright for those images is with original publishers, we are reusing with gratitute.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stfC7ldv6nmb"
      },
      "source": [
        "# **Infrastructure Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MZwaRkbVcNw"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yBJ5ziJmVYp0",
        "outputId": "d3aa770c-4c78-4c11-c89b-5463e2f3d9a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (0.3.56)\n",
            "Requirement already satisfied: langchain-groq in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (0.3.2)\n",
            "Requirement already satisfied: langgraph in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (0.3.34)\n",
            "Requirement already satisfied: arxiv in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (2.2.0)\n",
            "Requirement already satisfied: PyPDF2 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (3.0.1)\n",
            "Requirement already satisfied: requests in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (2.32.3)\n",
            "Requirement already satisfied: langchain-openai in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (0.3.14)\n",
            "Requirement already satisfied: python-dotenv in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain) (0.3.38)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain-core) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain-groq) (0.23.1)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langgraph) (2.0.25)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.8 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langgraph) (0.1.63)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain-openai) (1.76.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sgmllib3k in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.2)\n",
            "Requirement already satisfied: sniffio in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: tqdm>4 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/tchavva/Projects/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-core langchain-groq langgraph arxiv PyPDF2 requests langchain-openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmeBWd33Vekn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rPPjuB-tVg7M"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from typing import List, Dict, Any, Optional\n",
        "import arxiv\n",
        "from langchain.agents.agent import AgentExecutor, AgentAction, AgentFinish\n",
        "from langchain.agents import create_openai_tools_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.tools import Tool, StructuredTool\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "#from langgraph.prebuilt import ToolExecutor\n",
        "import requests\n",
        "import PyPDF2\n",
        "from io import BytesIO\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYkvzwi2Z4Ct",
        "outputId": "002da780-230b-4e42-8dfc-ec6057239217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq API key loaded successfully!\n",
            "OpenAI API key loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get the Groq API key\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Check if the key was found\n",
        "if not groq_api_key:\n",
        "    raise ValueError(\"GROQ_API_KEY not found in .env file\")\n",
        "\n",
        "print(\"Groq API key loaded successfully!\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
        "\n",
        "print(\"OpenAI API key loaded successfully!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZkflG4RVo-8"
      },
      "source": [
        "## Tools\n",
        "\n",
        "Let us create a set of tools which we will give to our LLM to use during its ReAct loop for the agent.\n",
        "\n",
        "\n",
        "We will keep using these tools again and again, to build the patterns called out above. This illustrates the reusability of the tools in AI Application building processes.\n",
        "\n",
        "In a enterprise / production environment - tools will be your existing internal APIs that are datasources that you would like to include as tools in our AI app.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oY1RXAdwVrOT"
      },
      "outputs": [],
      "source": [
        "# 1. Simple ArXiv Search Tool\n",
        "def search_arxiv(query, max_results=5):\n",
        "    \"\"\"Search for papers on arXiv and return basic info.\"\"\"\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(query=query, max_results=max_results)\n",
        "\n",
        "    results = []\n",
        "    for paper in client.results(search):\n",
        "        results.append({\n",
        "            \"id\": paper.entry_id.split(\"/\")[-1],\n",
        "            \"title\": paper.title,\n",
        "            \"authors\": [author.name for author in paper.authors],\n",
        "            \"url\": paper.pdf_url\n",
        "        })\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G4e4Ip1uVt1S"
      },
      "outputs": [],
      "source": [
        "# 2. Simple Paper Downloader\n",
        "def download_paper(paper_id):\n",
        "    \"\"\"Download a paper from arXiv by ID and extract text.\"\"\"\n",
        "    search = arxiv.Search(id_list=[paper_id])\n",
        "    client = arxiv.Client()\n",
        "    paper = next(client.results(search))\n",
        "\n",
        "    response = requests.get(paper.pdf_url)\n",
        "    pdf_file = BytesIO(response.content)\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "\n",
        "    return text[:5000]  # Return first 5000 chars for simplicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t7oOCwDpVwZl"
      },
      "outputs": [],
      "source": [
        "# 3. Simple Summarizer\n",
        "def summarize_text(text, groq_api_key):\n",
        "    \"\"\"Summarize text using Groq.\"\"\"\n",
        "    model = ChatGroq(api_key=groq_api_key, model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
        "    prompt = f\"Summarize this academic paper: {text[:15000]}\"\n",
        "    return model.invoke(prompt).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4_T9YDXHV2FA"
      },
      "outputs": [],
      "source": [
        "def create_tools(groq_api_key: str):\n",
        "    \"\"\"Create the tools for our agent.\"\"\"\n",
        "\n",
        "    tools = [\n",
        "        Tool(\n",
        "            name=\"search_arxiv\",\n",
        "            description=\"Search for academic papers on arXiv. Input is a search query.\",\n",
        "            func=lambda q: search_arxiv(q)\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"download_paper\",\n",
        "            description=\"Download a paper from arXiv and extract its text. Input is a paper ID.\",\n",
        "            func=lambda id: download_paper(id)\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"summarize_text\",\n",
        "            description=\"Summarize a piece of text. Input is the text to summarize.\",\n",
        "            func=lambda t: summarize_text(t, groq_api_key)\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oyNDKE2V3ZT"
      },
      "source": [
        "## The Augmented Model\n",
        "\n",
        "![Augmented LLM](./demo_pics/augmented_llm.png)\n",
        "This is the fundamental building block for building an Agents. The LLM is supplimented with tools and memory. Since we are using LnagChain as our framework, memory construct is inbuilt for us. We are treating search as equivalent to retreival in this case.\n",
        "\n",
        "Let us build a simple version of this pattern.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "jtTO0dKNXbAj"
      },
      "outputs": [],
      "source": [
        "# First, create a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant that can use tools to answer user questions about research papers.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])\n",
        "\n",
        "def setup_augmented_llm(groq_api_key):\n",
        "    # Define the tools\n",
        "    tools = create_tools(groq_api_key)\n",
        "\n",
        "    # Create the LLM\n",
        "    llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "\n",
        "    # Create the agent with tools\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "    # Create an agent executor\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "    return agent_executor\n",
        "\n",
        "# 5. Run the system\n",
        "def run_augmented_llm(query, groq_api_key):\n",
        "    agent_executor = setup_augmented_llm(groq_api_key)\n",
        "    result = agent_executor.invoke({\"input\": query})\n",
        "    return result[\"output\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufDX-MpdPr93"
      },
      "source": [
        "#### The Augmented LLM - Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pY8KEpaPwqR",
        "outputId": "05f444df-0a78-43e5-f39f-601a3494efa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `search_arxiv` with `latest developments quantum computing`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[{'id': '0305472v2', 'title': 'Arbitrage risk induced by transaction costs', 'authors': ['E. W. Piotrowski', 'J. Sladkowski'], 'url': 'http://arxiv.org/pdf/cond-mat/0305472v2'}, {'id': '2302.00001v1', 'title': 'A Living Review of Quantum Computing for Plasma Physics', 'authors': ['Óscar Amaro', 'Diogo Cruz'], 'url': 'http://arxiv.org/pdf/2302.00001v1'}, {'id': '2502.08925v1', 'title': 'Quantum Software Engineering and Potential of Quantum Computing in Software Engineering Research: A Review', 'authors': ['Ashis Kumar Mandal', 'Md Nadim', 'Chanchal K. Roy', 'Banani Roy', 'Kevin A. Schneider'], 'url': 'http://arxiv.org/pdf/2502.08925v1'}, {'id': '2407.16296v1', 'title': 'Quantum Computing for Climate Resilience and Sustainability Challenges', 'authors': ['Kin Tung Michael Ho', 'Kuan-Cheng Chen', 'Lily Lee', 'Felix Burt', 'Shang Yu', 'Po-Heng', 'Lee'], 'url': 'http://arxiv.org/pdf/2407.16296v1'}, {'id': '1912.00850v1', 'title': 'QFT with FDR', 'authors': ['Roberto Pittau'], 'url': 'http://arxiv.org/pdf/1912.00850v1'}]\u001b[0m\u001b[32;1m\u001b[1;3mRecent developments in quantum computing span improvements in hardware, error correction protocols, algorithm design, and software engineering. Here are some highlights:\n",
            "\n",
            "1. Hardware Advances:  \n",
            "• Researchers are making strides with various qubit platforms such as superconducting circuits, trapped ions, and photonic systems. These improvements focus on enhancing qubit coherence times and reducing error rates—a key requirement for scaling up quantum processors.  \n",
            "• There’s ongoing work on integrating diverse quantum architectures to combine the advantages of each system.\n",
            "\n",
            "2. Quantum Error Correction:  \n",
            "• Progress in error correction is vital for moving beyond Noisy Intermediate-Scale Quantum (NISQ) devices. New codes—such as those based on surface codes and topological approaches—are being developed and experimentally validated, paving the way toward fault-tolerant quantum computation.\n",
            "\n",
            "3. Algorithm and Application Development:  \n",
            "• On the algorithm side, researchers are expanding quantum algorithm portfolios to address problems in optimization, simulation, and machine learning. Variational quantum algorithms, in particular, are being explored for their potential in tackling real-world issues within the limitations of current hardware.  \n",
            "• Recent review articles illustrate the trend toward applying quantum computing in interdisciplinary fields. For instance, there are emerging applications in plasma physics, climate resilience, and sustainability challenges. These works review how quantum computing can offer new perspectives and methods for simulating complex physical systems.\n",
            "\n",
            "4. Quantum Software Engineering:  \n",
            "• The development of quantum compilers, languages, and simulation frameworks is gaining momentum. These tools aim to streamline the translation of abstract quantum algorithms into executable quantum circuits.  \n",
            "• Surveys in quantum software engineering are assessing how traditional software development practices can be adapted or rethought to accommodate the probabilistic nature of quantum computing.\n",
            "\n",
            "Overall, the quantum computing picture is rapidly evolving—from fundamental improvements in hardware and error correction to the exploration of novel applications across scientific disciplines. These developments not only push the boundaries of what quantum computers can achieve but also lay the groundwork for scalable, fault-tolerant quantum systems in the future.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Recent developments in quantum computing span improvements in hardware, error correction protocols, algorithm design, and software engineering. Here are some highlights:\n",
            "\n",
            "1. Hardware Advances:  \n",
            "• Researchers are making strides with various qubit platforms such as superconducting circuits, trapped ions, and photonic systems. These improvements focus on enhancing qubit coherence times and reducing error rates—a key requirement for scaling up quantum processors.  \n",
            "• There’s ongoing work on integrating diverse quantum architectures to combine the advantages of each system.\n",
            "\n",
            "2. Quantum Error Correction:  \n",
            "• Progress in error correction is vital for moving beyond Noisy Intermediate-Scale Quantum (NISQ) devices. New codes—such as those based on surface codes and topological approaches—are being developed and experimentally validated, paving the way toward fault-tolerant quantum computation.\n",
            "\n",
            "3. Algorithm and Application Development:  \n",
            "• On the algorithm side, researchers are expanding quantum algorithm portfolios to address problems in optimization, simulation, and machine learning. Variational quantum algorithms, in particular, are being explored for their potential in tackling real-world issues within the limitations of current hardware.  \n",
            "• Recent review articles illustrate the trend toward applying quantum computing in interdisciplinary fields. For instance, there are emerging applications in plasma physics, climate resilience, and sustainability challenges. These works review how quantum computing can offer new perspectives and methods for simulating complex physical systems.\n",
            "\n",
            "4. Quantum Software Engineering:  \n",
            "• The development of quantum compilers, languages, and simulation frameworks is gaining momentum. These tools aim to streamline the translation of abstract quantum algorithms into executable quantum circuits.  \n",
            "• Surveys in quantum software engineering are assessing how traditional software development practices can be adapted or rethought to accommodate the probabilistic nature of quantum computing.\n",
            "\n",
            "Overall, the quantum computing picture is rapidly evolving—from fundamental improvements in hardware and error correction to the exploration of novel applications across scientific disciplines. These developments not only push the boundaries of what quantum computers can achieve but also lay the groundwork for scalable, fault-tolerant quantum systems in the future.\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the latest developments in quantum computing?\"\n",
        "response = run_augmented_llm(query, groq_api_key)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZgHyK06i51-"
      },
      "source": [
        "## Prompt Chaining\n",
        "\n",
        "![Prompt Chaining](./demo_pics/prompt_chain.png)\n",
        "\n",
        "This workflow pattern chains output of one LLM Call / Tool to the other one.\n",
        "\n",
        "The research assistant is the orchestrator that coordinates the search and synthesis. search_llm and synthesis_llm are the two LLMs in this workflow. The output of search_llm is fed into the input of synthesis_llm. search_llm is used to convert the user's question into a search query using openai. synthesis_llm is used to synthesize the results of the search using groq.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgZcTuQapDSP"
      },
      "source": [
        "### Setup Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "JTxnmYL1nkGj"
      },
      "outputs": [],
      "source": [
        "def setup_prompt_chain(groq_api_key, openai_api_key):\n",
        "    \"\"\"\n",
        "    Setup a chain of prompts where the output of one LLM call is fed into another.\n",
        "    This demonstrates the Prompt Chaining pattern.\n",
        "    \"\"\"\n",
        "    # Create the LLMs\n",
        "    search_llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "    synthesis_llm = ChatGroq(api_key=groq_api_key, model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
        "\n",
        "    # Create the tools\n",
        "    tools = create_tools(groq_api_key)\n",
        "\n",
        "    # STEP 1: Create the search agent prompt\n",
        "    search_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a research assistant that helps find relevant papers on arXiv.\n",
        "Your job is to convert a user's research question into the best possible search query for arXiv.\n",
        "Focus on extracting key technical terms and concepts.\n",
        "Respond ONLY with the search query, nothing else.\"\"\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "\n",
        "    # STEP 2: Create the paper selection agent prompt\n",
        "    selection_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a research assistant that helps select the most relevant paper from search results.\n",
        "Given a list of papers and the original research question, select the SINGLE most relevant paper ID.\n",
        "Respond ONLY with the paper ID, nothing else.\"\"\"),\n",
        "        (\"human\", \"Research question: {original_query}\\nSearch results: {search_results}\")\n",
        "    ])\n",
        "\n",
        "    # STEP 3: Create the analysis agent prompt\n",
        "    analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a research expert that analyzes academic papers.\n",
        "Given the text of a paper, identify the key methodologies, findings, and implications.\n",
        "Focus on how this paper addresses the user's original question.\"\"\"),\n",
        "        (\"human\", \"Original question: {original_query}\\nPaper text: {paper_text}\")\n",
        "    ])\n",
        "\n",
        "    # STEP 4: Create the final synthesis prompt\n",
        "    synthesis_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a helpful AI research assistant that synthesizes information into clear,\n",
        "concise responses. Create a comprehensive yet accessible answer to the user's question based on the\n",
        "technical analysis provided.\"\"\"),\n",
        "        (\"human\", \"Original question: {original_query}\\nTechnical analysis: {analysis}\")\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        \"search_llm\": search_llm,\n",
        "        \"synthesis_llm\": synthesis_llm,\n",
        "        \"tools\": tools,\n",
        "        \"prompts\": {\n",
        "            \"search\": search_prompt,\n",
        "            \"selection\": selection_prompt,\n",
        "            \"analysis\": analysis_prompt,\n",
        "            \"synthesis\": synthesis_prompt\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwMNfyscpHjT"
      },
      "source": [
        "### Setup Chaining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "rZa050dLnnkY"
      },
      "outputs": [],
      "source": [
        "def run_prompt_chain(query, groq_api_key, openai_api_key):\n",
        "    \"\"\"\n",
        "    Execute the entire prompt chain to answer a research question.\n",
        "    This demonstrates how outputs from one LLM call are fed as inputs to the next.\n",
        "    \"\"\"\n",
        "    # Setup the chain components\n",
        "    chain = setup_prompt_chain(groq_api_key, openai_api_key)\n",
        "    search_llm = chain[\"search_llm\"]\n",
        "    synthesis_llm = chain[\"synthesis_llm\"]\n",
        "    tools = chain[\"tools\"]\n",
        "    prompts = chain[\"prompts\"]\n",
        "\n",
        "    print(\"📝 STARTING PROMPT CHAIN\")\n",
        "    print(f\"📋 Original query: {query}\")\n",
        "\n",
        "    # STEP 1: Convert question to search query\n",
        "    print(\"\\n🔍 Step 1: Converting question to optimal search query...\")\n",
        "    search_query = search_llm.invoke(prompts[\"search\"].format(input=query)).content\n",
        "    print(f\"🔍 Generated search query: {search_query}\")\n",
        "\n",
        "    # STEP 2: Search for papers\n",
        "    print(\"\\n📚 Step 2: Searching for papers on arXiv...\")\n",
        "    search_results = search_arxiv(search_query)\n",
        "    print(f\"📚 Found {len(search_results)} papers\")\n",
        "\n",
        "    # STEP 3: Select the most relevant paper\n",
        "    print(\"\\n🔎 Step 3: Selecting the most relevant paper...\")\n",
        "    paper_id = search_llm.invoke(\n",
        "        prompts[\"selection\"].format(\n",
        "            original_query=query,\n",
        "            search_results=search_results\n",
        "        )\n",
        "    ).content\n",
        "    print(f\"🔎 Selected paper ID: {paper_id}\")\n",
        "\n",
        "    # STEP 4: Download the selected paper\n",
        "    print(\"\\n📄 Step 4: Downloading the selected paper...\")\n",
        "    paper_text = download_paper(paper_id)\n",
        "    print(f\"📄 Downloaded {len(paper_text)} characters of paper text\")\n",
        "\n",
        "    # STEP 5: Analyze the paper\n",
        "    print(\"\\n🧪 Step 5: Analyzing the paper...\")\n",
        "    analysis = search_llm.invoke(\n",
        "        prompts[\"analysis\"].format(\n",
        "            original_query=query,\n",
        "            paper_text=paper_text\n",
        "        )\n",
        "    ).content\n",
        "    print(f\"🧪 Analysis complete\")\n",
        "\n",
        "    # STEP 6: Final synthesis\n",
        "    print(\"\\n🔮 Step 6: Synthesizing final response...\")\n",
        "    final_response = synthesis_llm.invoke(\n",
        "        prompts[\"synthesis\"].format(\n",
        "            original_query=query,\n",
        "            analysis=analysis\n",
        "        )\n",
        "    ).content\n",
        "\n",
        "    print(\"\\n✅ PROMPT CHAIN COMPLETE\")\n",
        "\n",
        "    return {\n",
        "        \"search_query\": search_query,\n",
        "        \"selected_paper\": next((p for p in search_results if p[\"id\"] == paper_id), None),\n",
        "        \"analysis\": analysis,\n",
        "        \"response\": final_response\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxsG3eqfnqPT",
        "outputId": "d8b6adf4-73a8-4a96-bd6b-7310ecf023d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📝 STARTING PROMPT CHAIN\n",
            "📋 Original query: What are the latest breakthroughs in quantum error correction?\n",
            "\n",
            "🔍 Step 1: Converting question to optimal search query...\n",
            "🔍 Generated search query: quantum error correction latest breakthroughs recent developments\n",
            "\n",
            "📚 Step 2: Searching for papers on arXiv...\n",
            "📚 Found 5 papers\n",
            "\n",
            "🔎 Step 3: Selecting the most relevant paper...\n",
            "🔎 Selected paper ID: 2412.20380v1\n",
            "\n",
            "📄 Step 4: Downloading the selected paper...\n",
            "📄 Downloaded 5000 characters of paper text\n",
            "\n",
            "🧪 Step 5: Analyzing the paper...\n",
            "🧪 Analysis complete\n",
            "\n",
            "🔮 Step 6: Synthesizing final response...\n",
            "\n",
            "✅ PROMPT CHAIN COMPLETE\n",
            "\n",
            "----- FINAL RESPONSE -----\n",
            "The latest breakthroughs in quantum error correction (QEC) are driven by the integration of artificial intelligence (AI) and machine learning (ML) techniques with traditional QEC methods. Recent studies, as highlighted in a comprehensive survey of over 150 papers, have shown that AI-driven QEC approaches outperform conventional methods in terms of efficiency and accuracy, particularly in handling the dynamic nature of errors in Noisy Intermediate-Scale Quantum (NISQ) systems.\n",
            "\n",
            "Key advancements include:\n",
            "\n",
            "1. **Improved error correction performance**: AI-driven QEC methods have demonstrated superior performance in detecting and correcting quantum errors, paving the way for more reliable quantum computing.\n",
            "2. **Scalable quantum computing**: Breakthroughs like Google's Willow processor and China's Zuchongzhi 3.0 quantum processor have shown exponential improvements in error-corrected qubits as the system scales, representing a significant milestone in the pursuit of stable and scalable quantum computing.\n",
            "3. **Practical implementation**: These advancements underscore a trend toward the practical implementation of QEC in real-world quantum devices, demonstrating the potential of these new approaches.\n",
            "\n",
            "The integration of AI and ML with QEC has profound implications for the future of quantum computing. By improving error correction pipelines, these techniques can help overcome current limitations, such as resource-intensive syndrome measurements and decoding algorithms, ultimately accelerating progress toward fault-tolerant quantum computing.\n",
            "\n",
            "However, challenges remain, particularly concerning data sparsity and scalability, which are identified as critical areas for future research. Further investigation into these areas is necessary to fully harness the potential of AI-driven QEC methods and to overcome the current limitations in scaling up quantum computing systems.\n",
            "\n",
            "In summary, the latest breakthroughs in quantum error correction are emerging from the convergence of AI-based techniques with traditional QEC frameworks, offering a pathway to overcoming longstanding challenges in scaling up quantum computing systems and paving the way for more reliable and scalable quantum computing.\n",
            "\n",
            "--------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'search_query': 'quantum error correction latest breakthroughs recent developments',\n",
              " 'selected_paper': {'id': '2412.20380v1',\n",
              "  'title': 'Artificial Intelligence for Quantum Error Correction: A Comprehensive Review',\n",
              "  'authors': ['Zihao Wang', 'Hao Tang'],\n",
              "  'url': 'http://arxiv.org/pdf/2412.20380v1'},\n",
              " 'analysis': \"Below is an analysis that highlights the paper’s methodologies, key findings, and implications with a focus on addressing breakthroughs in quantum error correction (QEC):\\n\\n1. Methodologies:\\n\\u2003• The paper is a comprehensive survey that reviews over 150 studies, with a particular focus on the integration of artificial intelligence (AI) and machine learning (ML) techniques into QEC.\\n\\u2003• It categorizes the AI/ML methods into several groups—unsupervised, supervised, semi-supervised, and reinforcement learning—exploring how these techniques enhance the detection and correction of quantum errors.\\n\\u2003• A comparative approach is used: traditional QEC methods such as Shor codes and surface codes are delineated and contrasted with newer AI-driven strategies, thereby contextualizing the innovations within established frameworks.\\n\\n2. Key Findings:\\n\\u2003• AI-driven QEC methods have shown superior efficiency and accuracy compared to conventional methods, particularly in handling the dynamic nature of errors in Noisy Intermediate Scale Quantum (NISQ) systems.\\n\\u2003• The survey highlights groundbreaking developments like Google’s Willow processor, where error-corrected qubits exhibit exponential improvement as the system scales. This represents a significant milestone in the pursuit of stable and scalable quantum computing.\\n\\u2003• Similarly, the paper discusses China's Zuchongzhi 3.0 quantum processor, which leverages advanced QEC techniques to achieve quantum computational advantages—demonstrating the practical potential of these new approaches.\\n\\u2003• Challenges remain, particularly concerning data sparsity and scalability, which the review identifies as critical areas for future research.\\n\\n3. Implications:\\n\\u2003• The integration of AI in QEC has profound implications for the future of quantum computing. By improving error correction pipelines, these techniques pave the way for more reliable and scalable quantum systems.\\n\\u2003• The breakthroughs mentioned (Google’s Willow processor and China’s Zuchongzhi 3.0) underscore a trend toward the practical implementation of QEC in real-world quantum devices.\\n\\u2003• The survey also suggests that deeper integration of AI and ML with quantum error correction strategies could help overcome current limitations such as resource-intensive syndrome measurements and decoding algorithms, ultimately accelerating progress toward fault-tolerant quantum computing.\\n\\nIn summary, the paper addresses the original question by showcasing how the latest breakthroughs in quantum error correction are emerging from the convergence of AI-based techniques with traditional QEC frameworks. These breakthroughs not only improve error correction performance but also offer a pathway to overcoming longstanding challenges in scaling up quantum computing systems.\",\n",
              " 'response': \"The latest breakthroughs in quantum error correction (QEC) are driven by the integration of artificial intelligence (AI) and machine learning (ML) techniques with traditional QEC methods. Recent studies, as highlighted in a comprehensive survey of over 150 papers, have shown that AI-driven QEC approaches outperform conventional methods in terms of efficiency and accuracy, particularly in handling the dynamic nature of errors in Noisy Intermediate-Scale Quantum (NISQ) systems.\\n\\nKey advancements include:\\n\\n1. **Improved error correction performance**: AI-driven QEC methods have demonstrated superior performance in detecting and correcting quantum errors, paving the way for more reliable quantum computing.\\n2. **Scalable quantum computing**: Breakthroughs like Google's Willow processor and China's Zuchongzhi 3.0 quantum processor have shown exponential improvements in error-corrected qubits as the system scales, representing a significant milestone in the pursuit of stable and scalable quantum computing.\\n3. **Practical implementation**: These advancements underscore a trend toward the practical implementation of QEC in real-world quantum devices, demonstrating the potential of these new approaches.\\n\\nThe integration of AI and ML with QEC has profound implications for the future of quantum computing. By improving error correction pipelines, these techniques can help overcome current limitations, such as resource-intensive syndrome measurements and decoding algorithms, ultimately accelerating progress toward fault-tolerant quantum computing.\\n\\nHowever, challenges remain, particularly concerning data sparsity and scalability, which are identified as critical areas for future research. Further investigation into these areas is necessary to fully harness the potential of AI-driven QEC methods and to overcome the current limitations in scaling up quantum computing systems.\\n\\nIn summary, the latest breakthroughs in quantum error correction are emerging from the convergence of AI-based techniques with traditional QEC frameworks, offering a pathway to overcoming longstanding challenges in scaling up quantum computing systems and paving the way for more reliable and scalable quantum computing.\"}"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage\n",
        "def run_example():\n",
        "    # Load API keys from environment\n",
        "    # groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    # openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Run the prompt chain\n",
        "    query = \"What are the latest breakthroughs in quantum error correction?\"\n",
        "    result = run_prompt_chain(query, groq_api_key, openai_api_key)\n",
        "\n",
        "    # Output the final response\n",
        "    print(\"\\n----- FINAL RESPONSE -----\")\n",
        "    print(result[\"response\"])\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return result\n",
        "\n",
        "run_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaCoumGbjWws"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbsouuK6q3LA"
      },
      "source": [
        "## Routing\n",
        "\n",
        "![Routing](./demo_pics/llm_router.png)\n",
        "\n",
        "Routing is a pattern where the agent analyzes the query and directs it to the appropriate specialized pathway.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "sDlEGLTJq98X"
      },
      "outputs": [],
      "source": [
        "class RouterAgent:\n",
        "    \"\"\"\n",
        "    A router that analyzes the query and directs it to the appropriate specialized pathway.\n",
        "    \"\"\"\n",
        "    def __init__(self, openai_api_key):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.router_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a query classifier that determines the type of research question.\n",
        "Analyze the user's research question and classify it into EXACTLY ONE of the following categories:\n",
        "1. \"latest_developments\" - Questions about recent advancements or state-of-the-art in a field\n",
        "2. \"deep_dive\" - Questions requiring in-depth analysis of specific techniques or methodologies\n",
        "3. \"comparison\" - Questions comparing different approaches, methods, or theories\n",
        "4. \"application\" - Questions about applying research to specific domains or problems\n",
        "\n",
        "Respond ONLY with the category name in lowercase, nothing else.\"\"\"),\n",
        "            (\"human\", \"{query}\")\n",
        "        ])\n",
        "\n",
        "    def route(self, query):\n",
        "        \"\"\"Determine which specialized pathway should handle this query.\"\"\"\n",
        "        response = self.llm.invoke(self.router_prompt.format(query=query))\n",
        "        query_type = response.content.strip().lower()\n",
        "\n",
        "        # Validate that the response is one of our expected categories\n",
        "        valid_types = [\"latest_developments\", \"deep_dive\", \"comparison\", \"application\"]\n",
        "        if query_type not in valid_types:\n",
        "            # Default to latest_developments if classification fails\n",
        "            query_type = \"latest_developments\"\n",
        "\n",
        "        return query_type\n",
        "\n",
        "class LatestDevelopmentsAgent:\n",
        "    \"\"\"Specialized agent for handling queries about recent developments.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in the latest developments and\n",
        "state-of-the-art research. Your goal is to find the most recent papers on a topic and\n",
        "summarize the cutting-edge advancements.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "class DeepDiveAgent:\n",
        "    \"\"\"Specialized agent for handling deep-dive queries.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in deep technical analysis.\n",
        "Your goal is to find and thoroughly analyze papers on specific techniques or methodologies.\n",
        "Focus on understanding the core principles, implementation details, and theoretical foundations.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "class ComparisonAgent:\n",
        "    \"\"\"Specialized agent for handling comparison queries.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in comparative analysis.\n",
        "Your goal is to find papers that compare different approaches, methods, or theories and\n",
        "analyze the strengths and weaknesses of each. Create a balanced assessment that highlights\n",
        "the key differences.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "class ApplicationAgent:\n",
        "    \"\"\"Specialized agent for handling application queries.\"\"\"\n",
        "    def __init__(self, openai_api_key, groq_api_key, tools):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.tools = tools\n",
        "        self.groq_api_key = groq_api_key\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant specializing in practical applications.\n",
        "Your goal is to find papers that demonstrate how research is applied to real-world problems\n",
        "and domains. Focus on implementation details, case studies, and practical outcomes.\"\"\"),\n",
        "            (\"human\", \"{query}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    def process(self, query):\n",
        "        agent = create_openai_tools_agent(self.llm, self.tools, self.prompt)\n",
        "        agent_executor = AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
        "        result = agent_executor.invoke({\"query\": query})\n",
        "        return result[\"output\"]\n",
        "\n",
        "def setup_routing_system(openai_api_key, groq_api_key):\n",
        "    \"\"\"Set up the complete routing system with specialized agents.\"\"\"\n",
        "    tools = create_tools(groq_api_key)\n",
        "\n",
        "    # Create the router and specialized agents\n",
        "    router = RouterAgent(openai_api_key)\n",
        "    agents = {\n",
        "        \"latest_developments\": LatestDevelopmentsAgent(openai_api_key, groq_api_key, tools),\n",
        "        \"deep_dive\": DeepDiveAgent(openai_api_key, groq_api_key, tools),\n",
        "        \"comparison\": ComparisonAgent(openai_api_key, groq_api_key, tools),\n",
        "        \"application\": ApplicationAgent(openai_api_key, groq_api_key, tools)\n",
        "    }\n",
        "\n",
        "    return router, agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RmiPXdlrYW8",
        "outputId": "65d19cea-c992-4538-b88b-2212b0aaa2d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚦 ROUTING SYSTEM INITIATED\n",
            "📋 Query: How do transformers handle attention mechanisms in deep learning?\n",
            "\n",
            "🔀 Routing query...\n",
            "🔀 Query classified as: deep_dive\n",
            "\n",
            "🔬 Processing with specialized deep_dive agent...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTransformers revolutionized deep learning by introducing the self-attention mechanism, which allows the model to weigh the importance of different tokens in the sequence when generating representations. Here’s a detailed look at how they handle attention:\n",
            "\n",
            "1. Self-Attention with Query, Key, and Value Vectors:\n",
            "   • Each input token is transformed into three vectors: a query (Q), a key (K), and a value (V) through learned linear projections.\n",
            "   • The attention mechanism computes a score between pairs of tokens by taking the dot product of a token's query with the key of another token. These scores indicate the relative importance or alignment between tokens.\n",
            "\n",
            "2. Scaled Dot-Product Attention:\n",
            "   • The raw dot-product scores are scaled by dividing by the square root of the dimension of the key vectors (usually denoted as √(dₖ)). This scaling helps mitigate issues with very large values in high-dimensional spaces.\n",
            "   • A softmax function is applied to these scaled scores to convert them into attention weights that sum to one.\n",
            "   • Finally, these weights are used to compute a weighted sum of the value vectors, producing a new representation for each token that incorporates information from other tokens in the sequence.\n",
            "\n",
            "3. Multi-Head Attention:\n",
            "   • Instead of computing a single attention distribution, transformers use multiple “heads.” Each head performs its own independent attention mechanism.\n",
            "   • These parallel attention heads allow the model to capture various types of relationships (for example, syntactic or semantic relationships) in different subspaces.\n",
            "   • The outputs of the multiple heads are concatenated and linearly transformed to produce the final representation.\n",
            "\n",
            "4. Positional Encoding:\n",
            "   • Since transformers lack the sequential inductive biases inherent in RNNs or CNNs, they add positional encodings to the input embeddings.\n",
            "   • These encodings inject information about the position of each token in the sequence, ensuring that the model can capture the order of the tokens.\n",
            "\n",
            "5. Integration in the Transformer Architecture:\n",
            "   • In the encoder, multi-head self-attention layers are followed by position-wise feed-forward networks. The entire process is wrapped with residual connections and layer normalization to stabilize training.\n",
            "   • In the decoder, transformer models often use a combination of self-attention (for the target sequence) and encoder-decoder attention (where queries come from the decoder and keys/values from the encoder) to generate outputs conditioned on the input sequence.\n",
            "\n",
            "By leveraging these methods, transformers can capture complex dependencies across different positions in the input sequence, making them highly effective for tasks like language modeling, translation, and beyond. The scalability and parallelizability of the attention mechanism contribute significantly to the power and efficiency of transformer architectures in deep learning.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "✅ ROUTING COMPLETE\n",
            "\n",
            "----- FINAL RESPONSE -----\n",
            "Transformers revolutionized deep learning by introducing the self-attention mechanism, which allows the model to weigh the importance of different tokens in the sequence when generating representations. Here’s a detailed look at how they handle attention:\n",
            "\n",
            "1. Self-Attention with Query, Key, and Value Vectors:\n",
            "   • Each input token is transformed into three vectors: a query (Q), a key (K), and a value (V) through learned linear projections.\n",
            "   • The attention mechanism computes a score between pairs of tokens by taking the dot product of a token's query with the key of another token. These scores indicate the relative importance or alignment between tokens.\n",
            "\n",
            "2. Scaled Dot-Product Attention:\n",
            "   • The raw dot-product scores are scaled by dividing by the square root of the dimension of the key vectors (usually denoted as √(dₖ)). This scaling helps mitigate issues with very large values in high-dimensional spaces.\n",
            "   • A softmax function is applied to these scaled scores to convert them into attention weights that sum to one.\n",
            "   • Finally, these weights are used to compute a weighted sum of the value vectors, producing a new representation for each token that incorporates information from other tokens in the sequence.\n",
            "\n",
            "3. Multi-Head Attention:\n",
            "   • Instead of computing a single attention distribution, transformers use multiple “heads.” Each head performs its own independent attention mechanism.\n",
            "   • These parallel attention heads allow the model to capture various types of relationships (for example, syntactic or semantic relationships) in different subspaces.\n",
            "   • The outputs of the multiple heads are concatenated and linearly transformed to produce the final representation.\n",
            "\n",
            "4. Positional Encoding:\n",
            "   • Since transformers lack the sequential inductive biases inherent in RNNs or CNNs, they add positional encodings to the input embeddings.\n",
            "   • These encodings inject information about the position of each token in the sequence, ensuring that the model can capture the order of the tokens.\n",
            "\n",
            "5. Integration in the Transformer Architecture:\n",
            "   • In the encoder, multi-head self-attention layers are followed by position-wise feed-forward networks. The entire process is wrapped with residual connections and layer normalization to stabilize training.\n",
            "   • In the decoder, transformer models often use a combination of self-attention (for the target sequence) and encoder-decoder attention (where queries come from the decoder and keys/values from the encoder) to generate outputs conditioned on the input sequence.\n",
            "\n",
            "By leveraging these methods, transformers can capture complex dependencies across different positions in the input sequence, making them highly effective for tasks like language modeling, translation, and beyond. The scalability and parallelizability of the attention mechanism contribute significantly to the power and efficiency of transformer architectures in deep learning.\n",
            "\n",
            "--------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query_type': 'deep_dive',\n",
              " 'response': \"Transformers revolutionized deep learning by introducing the self-attention mechanism, which allows the model to weigh the importance of different tokens in the sequence when generating representations. Here’s a detailed look at how they handle attention:\\n\\n1. Self-Attention with Query, Key, and Value Vectors:\\n   • Each input token is transformed into three vectors: a query (Q), a key (K), and a value (V) through learned linear projections.\\n   • The attention mechanism computes a score between pairs of tokens by taking the dot product of a token's query with the key of another token. These scores indicate the relative importance or alignment between tokens.\\n\\n2. Scaled Dot-Product Attention:\\n   • The raw dot-product scores are scaled by dividing by the square root of the dimension of the key vectors (usually denoted as √(dₖ)). This scaling helps mitigate issues with very large values in high-dimensional spaces.\\n   • A softmax function is applied to these scaled scores to convert them into attention weights that sum to one.\\n   • Finally, these weights are used to compute a weighted sum of the value vectors, producing a new representation for each token that incorporates information from other tokens in the sequence.\\n\\n3. Multi-Head Attention:\\n   • Instead of computing a single attention distribution, transformers use multiple “heads.” Each head performs its own independent attention mechanism.\\n   • These parallel attention heads allow the model to capture various types of relationships (for example, syntactic or semantic relationships) in different subspaces.\\n   • The outputs of the multiple heads are concatenated and linearly transformed to produce the final representation.\\n\\n4. Positional Encoding:\\n   • Since transformers lack the sequential inductive biases inherent in RNNs or CNNs, they add positional encodings to the input embeddings.\\n   • These encodings inject information about the position of each token in the sequence, ensuring that the model can capture the order of the tokens.\\n\\n5. Integration in the Transformer Architecture:\\n   • In the encoder, multi-head self-attention layers are followed by position-wise feed-forward networks. The entire process is wrapped with residual connections and layer normalization to stabilize training.\\n   • In the decoder, transformer models often use a combination of self-attention (for the target sequence) and encoder-decoder attention (where queries come from the decoder and keys/values from the encoder) to generate outputs conditioned on the input sequence.\\n\\nBy leveraging these methods, transformers can capture complex dependencies across different positions in the input sequence, making them highly effective for tasks like language modeling, translation, and beyond. The scalability and parallelizability of the attention mechanism contribute significantly to the power and efficiency of transformer architectures in deep learning.\"}"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def run_routing_system(query, openai_api_key, groq_api_key):\n",
        "    \"\"\"Run the routing system end-to-end.\"\"\"\n",
        "    router, agents = setup_routing_system(openai_api_key, groq_api_key)\n",
        "\n",
        "    print(\"🚦 ROUTING SYSTEM INITIATED\")\n",
        "    print(f\"📋 Query: {query}\")\n",
        "\n",
        "    # Step 1: Route the query\n",
        "    print(\"\\n🔀 Routing query...\")\n",
        "    query_type = router.route(query)\n",
        "    print(f\"🔀 Query classified as: {query_type}\")\n",
        "\n",
        "    # Step 2: Process with the appropriate agent\n",
        "    print(f\"\\n🔬 Processing with specialized {query_type} agent...\")\n",
        "    response = agents[query_type].process(query)\n",
        "\n",
        "    print(\"\\n✅ ROUTING COMPLETE\")\n",
        "\n",
        "    return {\n",
        "        \"query_type\": query_type,\n",
        "        \"response\": response\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "def run_example():\n",
        "    # Load API keys from environment\n",
        "    # groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    # openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Example queries for different types\n",
        "    example_queries = {\n",
        "        \"latest_developments\": \"What are the latest advancements in quantum computing?\",\n",
        "        \"deep_dive\": \"How do transformers handle attention mechanisms in deep learning?\",\n",
        "        \"comparison\": \"Compare LSTM and transformer approaches for sequence modeling\",\n",
        "        \"application\": \"How is reinforcement learning applied to robotics?\"\n",
        "    }\n",
        "\n",
        "    # Choose one query to run\n",
        "    query = example_queries[\"deep_dive\"]\n",
        "\n",
        "    # Run the routing system\n",
        "    result = run_routing_system(query, openai_api_key, groq_api_key)\n",
        "\n",
        "    # Output the final response\n",
        "    print(\"\\n----- FINAL RESPONSE -----\")\n",
        "    print(result[\"response\"])\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return result\n",
        "\n",
        "run_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIP5opPY0pTM"
      },
      "source": [
        "## The Agent\n",
        "\n",
        "![Anthropic Prompt Chaining](https://drive.google.com/uc?export=view&id=1_ERsFpGuxGUq_lkfuoW0O5dc9nrSXoSy)\n",
        "\n",
        "Most flexible but difficult to get it right pattern, where you are relying on LLM (usually more than one) to provide a reasoned plan of action, observe and evaluate the results of the actions from environment, reason and decide on next steps including stopping the computation.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7BYczEQ1Z5n"
      },
      "source": [
        "LLM as Orchstrator & Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bHOer1VU00a8"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional, Union\n",
        "class AgentAction(Enum):\n",
        "    \"\"\"Enum for different possible agent actions.\"\"\"\n",
        "    USE_TOOL = \"use_tool\"\n",
        "    ROUTE = \"route\"\n",
        "    FINAL_ANSWER = \"final_answer\"\n",
        "\n",
        "class ResearchPath(Enum):\n",
        "    \"\"\"Enum for different research paths.\"\"\"\n",
        "    OVERVIEW = \"overview\"\n",
        "    DEEP_DIVE = \"deep_dive\"\n",
        "    COMPARISON = \"comparison\"\n",
        "    TECHNICAL = \"technical\"\n",
        "\n",
        "# Define the schema for LLM response using Pydantic\n",
        "class ToolActionInput(BaseModel):\n",
        "    \"\"\"Schema for tool action input.\"\"\"\n",
        "    tool_name: str = Field(description=\"Name of the tool to use\")\n",
        "    tool_input: str = Field(description=\"Input to provide to the tool\")\n",
        "\n",
        "class RouteActionInput(BaseModel):\n",
        "    \"\"\"Schema for routing action input.\"\"\"\n",
        "    path: str = Field(description=\"Research path to take\")\n",
        "\n",
        "class FinalAnswerInput(BaseModel):\n",
        "    \"\"\"Schema for final answer input.\"\"\"\n",
        "    answer: str = Field(description=\"Final answer to the query\")\n",
        "\n",
        "class AgentDecision(BaseModel):\n",
        "    \"\"\"Schema for agent decision.\"\"\"\n",
        "    thought: str = Field(description=\"Detailed reasoning about the current state and what to do next\")\n",
        "    action: str = Field(description=\"Action to take: use_tool, route, or final_answer\")\n",
        "    action_input: Union[ToolActionInput, RouteActionInput, FinalAnswerInput] = Field(\n",
        "        description=\"Input for the action\"\n",
        "    )\n",
        "\n",
        "class AgentState:\n",
        "    \"\"\"State container for the agent's reasoning process.\"\"\"\n",
        "    def __init__(self, query: str):\n",
        "        self.query = query\n",
        "        self.current_path: Optional[ResearchPath] = None\n",
        "        self.tool_results: List[Dict[str, Any]] = []\n",
        "        self.working_memory: List[str] = []\n",
        "        self.final_answer: Optional[str] = None\n",
        "        self.conversation_history: List[Dict[str, Any]] = []\n",
        "\n",
        "    def add_tool_result(self, tool_name: str, tool_input: str, tool_output: Any) -> None:\n",
        "        \"\"\"Add the result of a tool call to the state.\"\"\"\n",
        "        self.tool_results.append({\n",
        "            \"tool_name\": tool_name,\n",
        "            \"tool_input\": tool_input,\n",
        "            \"tool_output\": tool_output\n",
        "        })\n",
        "\n",
        "    def add_to_memory(self, thought: str) -> None:\n",
        "        \"\"\"Add a thought to working memory.\"\"\"\n",
        "        self.working_memory.append(thought)\n",
        "\n",
        "    def set_path(self, path: ResearchPath) -> None:\n",
        "        \"\"\"Set the current research path.\"\"\"\n",
        "        self.current_path = path\n",
        "\n",
        "    def set_final_answer(self, answer: str) -> None:\n",
        "        \"\"\"Set the final answer.\"\"\"\n",
        "        self.final_answer = answer\n",
        "\n",
        "    def add_message(self, role: str, content: str) -> None:\n",
        "        \"\"\"Add a message to the conversation history.\"\"\"\n",
        "        self.conversation_history.append({\n",
        "            \"role\": role,\n",
        "            \"content\": content\n",
        "        })\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert the state to a dictionary for serialization.\"\"\"\n",
        "        return {\n",
        "            \"query\": self.query,\n",
        "            \"current_path\": self.current_path.value if self.current_path else None,\n",
        "            \"tool_results\": self.tool_results,\n",
        "            \"working_memory\": self.working_memory,\n",
        "            \"final_answer\": self.final_answer,\n",
        "        }\n",
        "\n",
        "class ResearchAgent:\n",
        "    \"\"\"An agent that can dynamically make decisions about research queries.\"\"\"\n",
        "\n",
        "    def __init__(self, openai_api_key: str, groq_api_key: str):\n",
        "        self.llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "        self.groq_api_key = groq_api_key\n",
        "        self.tools = create_tools(groq_api_key)\n",
        "\n",
        "        # Create the JSON output parser\n",
        "        self.parser = JsonOutputParser(pydantic_object=AgentDecision)\n",
        "\n",
        "        # Create the decision prompt template with LangChain's JSON parser format instructions\n",
        "        self.decision_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a research assistant agent capable of dynamically making decisions about how to answer research questions.\n",
        "You must explicitly reason through THREE types of decisions:\n",
        "\n",
        "1. ROUTING - Determine which research path would best address the query:\n",
        "   - OVERVIEW: For broad, introductory questions needing a general summary\n",
        "   - DEEP_DIVE: For questions requiring in-depth focus on a specific topic\n",
        "   - COMPARISON: For questions comparing multiple approaches or methods\n",
        "   - TECHNICAL: For questions requiring detailed technical explanations\n",
        "\n",
        "2. TOOL SELECTION - Determine which tool to use next from:\n",
        "   - search_arxiv: Search for academic papers (input: search query)\n",
        "   - download_paper: Get the text of a specific paper (input: paper ID)\n",
        "   - summarize_text: Summarize a piece of text (input: text to summarize)\n",
        "\n",
        "3. FINAL ANSWER - Determine if you have enough information to provide a final answer\n",
        "\n",
        "For each decision, you MUST use the following process:\n",
        "1. Think through the options step-by-step\n",
        "2. Consider the query and current state\n",
        "3. Make an explicit choice\n",
        "4. Explain your reasoning\n",
        "\n",
        "{format_instructions}\"\"\"),\n",
        "            (\"human\", \"\"\"\n",
        "QUERY: {query}\n",
        "\n",
        "CURRENT PATH: {current_path}\n",
        "\n",
        "TOOL RESULTS:\n",
        "{tool_results}\n",
        "\n",
        "WORKING MEMORY:\n",
        "{working_memory}\n",
        "\n",
        "Based on the above, what should I do next? Think through whether I should choose a research path, use a tool, or provide a final answer.\n",
        "\"\"\")\n",
        "        ])\n",
        "\n",
        "    def _format_tool_results(self, tool_results: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Format tool results for the prompt.\"\"\"\n",
        "        if not tool_results:\n",
        "            return \"No tools have been used yet.\"\n",
        "\n",
        "        result_strings = []\n",
        "        for i, result in enumerate(tool_results):\n",
        "            result_str = f\"Tool {i+1}: {result['tool_name']}\\n\"\n",
        "            result_str += f\"Input: {result['tool_input']}\\n\"\n",
        "\n",
        "            # Format the output depending on the tool\n",
        "            if result['tool_name'] == \"search_arxiv\":\n",
        "                papers = result['tool_output']\n",
        "                result_str += \"Output (Papers):\\n\"\n",
        "                for j, paper in enumerate(papers):\n",
        "                    result_str += f\"  Paper {j+1}: {paper['title']}\\n\"\n",
        "                    result_str += f\"  ID: {paper['id']}\\n\"\n",
        "                    result_str += f\"  Authors: {', '.join(paper['authors'])}\\n\"\n",
        "                    result_str += f\"  URL: {paper['url']}\\n\\n\"\n",
        "            elif result['tool_name'] == \"download_paper\":\n",
        "                # Truncate long paper texts\n",
        "                paper_text = result['tool_output']\n",
        "                if len(paper_text) > 500:\n",
        "                    paper_text = paper_text[:500] + \"... [text truncated]\"\n",
        "                result_str += f\"Output (Paper Text):\\n{paper_text}\\n\\n\"\n",
        "            elif result['tool_name'] == \"summarize_text\":\n",
        "                result_str += f\"Output (Summary):\\n{result['tool_output']}\\n\\n\"\n",
        "\n",
        "            result_strings.append(result_str)\n",
        "\n",
        "        return \"\\n\".join(result_strings)\n",
        "\n",
        "    def _format_working_memory(self, working_memory: List[str]) -> str:\n",
        "        \"\"\"Format working memory for the prompt.\"\"\"\n",
        "        if not working_memory:\n",
        "            return \"No thoughts recorded yet.\"\n",
        "\n",
        "        memory_strings = []\n",
        "        for i, thought in enumerate(working_memory):\n",
        "            memory_strings.append(f\"Thought {i+1}: {thought}\")\n",
        "\n",
        "        return \"\\n\".join(memory_strings)\n",
        "\n",
        "    def _get_next_action(self, state: AgentState) -> Dict[str, Any]:\n",
        "      \"\"\"Get the next action from the LLM.\"\"\"\n",
        "      # Format the state for the prompt\n",
        "      prompt_args = {\n",
        "          \"query\": state.query,\n",
        "          \"current_path\": state.current_path.value if state.current_path else \"Not selected yet\",\n",
        "          \"tool_results\": self._format_tool_results(state.tool_results),\n",
        "          \"working_memory\": self._format_working_memory(state.working_memory),\n",
        "          \"format_instructions\": self.parser.get_format_instructions()\n",
        "      }\n",
        "      \n",
        "      # Get the LLM's decision\n",
        "      response = self.llm.invoke(self.decision_prompt.format(**prompt_args))\n",
        "      \n",
        "      # Parse the JSON response\n",
        "      try:\n",
        "          # First, parse the raw text into a Python dictionary\n",
        "          response_text = response.content\n",
        "          \n",
        "          # Try to find JSON in the response\n",
        "          import re\n",
        "          import json\n",
        "          \n",
        "          # Look for JSON patterns\n",
        "          match = re.search(r'(\\{.*\\})', response_text, re.DOTALL)\n",
        "          if match:\n",
        "              json_str = match.group(1)\n",
        "              # Parse JSON string into Python dictionary\n",
        "              action_data = json.loads(json_str)\n",
        "              \n",
        "              # Now we have a dictionary with the parsed data\n",
        "              return {\n",
        "                  \"thought\": action_data.get(\"thought\", \"No reasoning provided\"),\n",
        "                  \"action\": action_data.get(\"action\", \"final_answer\"),\n",
        "                  \"action_input\": action_data.get(\"action_input\", {\"answer\": \"I don't have enough information to answer.\"})\n",
        "              }\n",
        "          else:\n",
        "              # If no JSON found\n",
        "              return {\n",
        "                  \"thought\": \"Failed to extract JSON from response\",\n",
        "                  \"action\": \"final_answer\",\n",
        "                  \"action_input\": {\n",
        "                      \"answer\": \"I encountered an error in my reasoning process.\"\n",
        "                  }\n",
        "              }\n",
        "              \n",
        "      except Exception as e:\n",
        "          # Better error handling\n",
        "          print(f\"Error parsing LLM response: {e}\")\n",
        "          print(f\"Problem response: {response_text}\")\n",
        "          \n",
        "          # Fall back to a predetermined structure\n",
        "          return {\n",
        "              \"thought\": f\"There was an error parsing my previous response: {str(e)}\",\n",
        "              \"action\": \"final_answer\",\n",
        "              \"action_input\": {\n",
        "                  \"answer\": \"I encountered a technical error in my reasoning process. Let me provide a direct answer based on my knowledge: Transformers and LSTMs differ primarily in how they process sequential data - LSTMs process data sequentially while transformers can process the entire sequence at once through self-attention mechanisms.\"\n",
        "              }\n",
        "          }\n",
        "\n",
        "    def _execute_tool(self, tool_name: str, tool_input: str) -> Any:\n",
        "        \"\"\"Execute a tool and return the result.\"\"\"\n",
        "        # Find the tool\n",
        "        tool = next((t for t in self.tools if t.name == tool_name), None)\n",
        "        if not tool:\n",
        "            return f\"Error: Tool '{tool_name}' not found.\"\n",
        "\n",
        "        # Execute the tool\n",
        "        try:\n",
        "            result = tool.func(tool_input)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return f\"Error executing tool: {str(e)}\"\n",
        "\n",
        "    def _route_query(self, path_name: str) -> ResearchPath:\n",
        "        \"\"\"Route the query to a research path.\"\"\"\n",
        "        try:\n",
        "            return ResearchPath(path_name)\n",
        "        except ValueError:\n",
        "            # Default to overview if invalid path\n",
        "            return ResearchPath.OVERVIEW\n",
        "\n",
        "    def process_query(self, query: str, max_steps: int = 10) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a research query using the agent's reasoning loop.\n",
        "\n",
        "        Args:\n",
        "            query: The research query to process\n",
        "            max_steps: Maximum number of reasoning steps to perform\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the final answer and the agent's state\n",
        "        \"\"\"\n",
        "        # Initialize the agent state\n",
        "        state = AgentState(query)\n",
        "\n",
        "        print(\"🤖 RESEARCH AGENT INITIATED\")\n",
        "        print(f\"📋 Query: {query}\")\n",
        "\n",
        "        # Main reasoning loop\n",
        "        for step in range(max_steps):\n",
        "            print(f\"\\n🔄 Step {step+1}: Reasoning...\")\n",
        "\n",
        "            # Get the next action\n",
        "            action_data = self._get_next_action(state)\n",
        "\n",
        "            # Add the thought to working memory\n",
        "            thought = action_data.get(\"thought\", \"No reasoning provided\")\n",
        "            state.add_to_memory(thought)\n",
        "            print(f\"💭 Thought: {thought}\")\n",
        "\n",
        "            # Parse the action\n",
        "            action = action_data.get(\"action\")\n",
        "            action_input = action_data.get(\"action_input\", {})\n",
        "\n",
        "            if action == AgentAction.USE_TOOL.value:\n",
        "                # Execute the tool\n",
        "                tool_name = action_input.get(\"tool_name\")\n",
        "                tool_input = action_input.get(\"tool_input\")\n",
        "\n",
        "                print(f\"🔧 Using Tool: {tool_name}\")\n",
        "                print(f\"📥 Tool Input: {tool_input}\")\n",
        "\n",
        "                tool_output = self._execute_tool(tool_name, tool_input)\n",
        "                state.add_tool_result(tool_name, tool_input, tool_output)\n",
        "\n",
        "                print(f\"📤 Tool Output: {type(tool_output)} with {len(str(tool_output))} chars\")\n",
        "\n",
        "            elif action == AgentAction.ROUTE.value:\n",
        "                # Set the research path\n",
        "                path_name = action_input.get(\"path\")\n",
        "                path = self._route_query(path_name)\n",
        "                state.set_path(path)\n",
        "\n",
        "                print(f\"🔀 Routing to Path: {path.value}\")\n",
        "\n",
        "            elif action == AgentAction.FINAL_ANSWER.value:\n",
        "                # Set the final answer\n",
        "                answer = action_input.get(\"answer\")\n",
        "                state.set_final_answer(answer)\n",
        "\n",
        "                print(f\"✅ Final Answer Ready\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                # Handle unknown action\n",
        "                print(f\"⚠️ Unknown Action: {action}\")\n",
        "                state.set_final_answer(f\"I encountered an error in my reasoning process. Unknown action: {action}\")\n",
        "                break\n",
        "        else:\n",
        "            # Handle max steps reached\n",
        "            state.set_final_answer(\"I've reached the maximum number of reasoning steps without finding a complete answer. Here's what I've learned so far: \" +\n",
        "                                 \"\\n\\n\".join(state.working_memory[-3:]))\n",
        "            print(\"⚠️ Maximum steps reached without final answer\")\n",
        "\n",
        "        print(\"\\n✨ REASONING COMPLETE\")\n",
        "\n",
        "        return {\n",
        "            \"final_answer\": state.final_answer,\n",
        "            \"state\": state.to_dict()\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnaJ5DyJ2VPm",
        "outputId": "3aa3baee-b97f-476f-afee-3b4e513ade80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 RESEARCH AGENT INITIATED\n",
            "📋 Query: What are the differences between transformers and LSTMs for NLP tasks?\n",
            "\n",
            "🔄 Step 1: Reasoning...\n",
            "Error parsing LLM response: 'dict' object has no attribute 'thought'\n",
            "Problem response: {\n",
            "  \"thought\": \"The query asks for a comparison between transformers and LSTMs for NLP tasks. This is clearly a COMPARISON type question. Given that I have sufficient background knowledge about these architectures, I can provide a comprehensive answer directly without needing to consult additional resources or external tools.\",\n",
            "  \"action\": \"final_answer\",\n",
            "  \"action_input\": {\n",
            "    \"answer\": \"Transformers and LSTMs differ significantly in their design and how they process sequential data for NLP tasks. Here are the key differences:\\n\\n1. Architectural Approach:\\n   - LSTMs (Long Short-Term Memory networks) are a type of recurrent neural network (RNN) that process data sequentially. They maintain and update a hidden state that evolves with each time step, which makes them inherently sequential.\\n   - Transformers, on the other hand, rely on self-attention mechanisms. Instead of processing data one element at a time, they process all tokens in parallel, allowing each token to attend to every other token in the sequence.\\n\\n2. Handling Long-Range Dependencies:\\n   - LSTMs are designed to capture long-term dependencies via gated mechanisms, but they are still subject to issues like vanishing gradients over very long sequences.\\n   - Transformers can directly capture long-range dependencies because the self-attention mechanism allows each token to consider every other token in the sequence without being constrained by sequential order.\\n\\n3. Parallelization and Efficiency:\\n   - Due to their sequential nature, LSTMs are harder to parallelize, which can lead to slower training times, especially on long sequences.\\n   - Transformers enable efficient parallelization since the self-attention mechanism processes all tokens simultaneously, making them more scalable and faster during training on modern hardware.\\n\\n4. Scalability and Data Requirements:\\n   - LSTMs can work well in scenarios with limited computational resources and data, but they often struggle to scale up for very large datasets compared to transformers.\\n   - Transformers tend to require large amounts of data to train effectively, but their architecture scales extremely well, as evidenced by models like BERT and GPT.\\n\\n5. Application Impact:\\n   - LSTMs have historically been used for various NLP tasks such as language modeling, machine translation, and sequence prediction.\\n   - Transformers have become the standard in many state-of-the-art NLP applications due to their superior ability to model contextual relationships, improved training efficiency, and scalability.\\n\\nIn summary, while both transformers and LSTMs have their own strengths, transformers offer advantages in terms of parallel processing, handling long-range dependencies, and scalability, making them the preferred choice in many contemporary NLP applications.\"\n",
            "  }\n",
            "}\n",
            "💭 Thought: There was an error parsing my previous response: 'dict' object has no attribute 'thought'\n",
            "✅ Final Answer Ready\n",
            "\n",
            "✨ REASONING COMPLETE\n",
            "\n",
            "----- FINAL ANSWER -----\n",
            "I encountered a technical error in my reasoning process. Let me provide a direct answer based on my knowledge: Transformers and LSTMs differ primarily in how they process sequential data - LSTMs process data sequentially while transformers can process the entire sequence at once through self-attention mechanisms.\n",
            "\n",
            "--------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'final_answer': 'I encountered a technical error in my reasoning process. Let me provide a direct answer based on my knowledge: Transformers and LSTMs differ primarily in how they process sequential data - LSTMs process data sequentially while transformers can process the entire sequence at once through self-attention mechanisms.',\n",
              " 'state': {'query': 'What are the differences between transformers and LSTMs for NLP tasks?',\n",
              "  'current_path': None,\n",
              "  'tool_results': [],\n",
              "  'working_memory': [\"There was an error parsing my previous response: 'dict' object has no attribute 'thought'\"],\n",
              "  'final_answer': 'I encountered a technical error in my reasoning process. Let me provide a direct answer based on my knowledge: Transformers and LSTMs differ primarily in how they process sequential data - LSTMs process data sequentially while transformers can process the entire sequence at once through self-attention mechanisms.'}}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage\n",
        "def run_example():\n",
        "    # Load API keys from environment\n",
        "    # groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    # openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Create the agent\n",
        "    agent = ResearchAgent(openai_api_key, groq_api_key)\n",
        "\n",
        "    # Example query\n",
        "    query = \"What are the differences between transformers and LSTMs for NLP tasks?\"\n",
        "\n",
        "    # Process the query\n",
        "    result = agent.process_query(query)\n",
        "\n",
        "    # Output the final answer\n",
        "    print(\"\\n----- FINAL ANSWER -----\")\n",
        "    print(result[\"final_answer\"])\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return result\n",
        "\n",
        "run_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOaAIXOE4_RH"
      },
      "source": [
        "## [Optional] A Succinct Version\n",
        "\n",
        "This implementation of the agent uses LangGraph framework to make the above verbose version (which was more for illustrative perposes) more succinct while not loosing any functionality.\n",
        "\n",
        "This goes on to show that frameworks like `langgraph` abstract common boiler plate code into the frameworks and provide important prebuilt constructs like OutputParser to simplify the agent building and working with LLMs.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "S8wXhFZB511R"
      },
      "outputs": [],
      "source": [
        "# ---- Define tool schemas for better structured input ----\n",
        "\n",
        "class SearchArxivInput(BaseModel):\n",
        "    \"\"\"Input schema for searching arXiv.\"\"\"\n",
        "    query: str = Field(description=\"The search query for arXiv papers\")\n",
        "    max_results: int = Field(default=5, description=\"Maximum number of results\")\n",
        "\n",
        "class DownloadPaperInput(BaseModel):\n",
        "    \"\"\"Input schema for downloading a paper.\"\"\"\n",
        "    paper_id: str = Field(description=\"The arXiv paper ID to download\")\n",
        "\n",
        "class SummarizeTextInput(BaseModel):\n",
        "    \"\"\"Input schema for text summarization.\"\"\"\n",
        "    text: str = Field(description=\"The text to summarize\")\n",
        "\n",
        "def create_structured_tools(groq_api_key: str) -> List[StructuredTool]:\n",
        "    \"\"\"Create structured tools for the agent.\"\"\"\n",
        "\n",
        "    search_arxiv_tool = StructuredTool.from_function(\n",
        "        func=search_arxiv,\n",
        "        name=\"search_arxiv\",\n",
        "        description=\"\"\"Search for academic papers on arXiv. This tool returns a list of papers\n",
        "            with their titles, IDs, authors, and URLs. Use this when you need to find papers on a topic.\"\"\",\n",
        "        args_schema=SearchArxivInput\n",
        "    )\n",
        "\n",
        "    download_paper_tool = StructuredTool.from_function(\n",
        "        func=download_paper,\n",
        "        name=\"download_paper\",\n",
        "        description=\"\"\"Download and extract the text content of an arXiv paper.\n",
        "            Provide the paper ID (just the number part). Use this after finding\n",
        "            papers with search_arxiv to read their content.\"\"\",\n",
        "        args_schema=DownloadPaperInput\n",
        "    )\n",
        "\n",
        "    summarize_text_tool = StructuredTool.from_function(\n",
        "        func=lambda text: summarize_text(text, groq_api_key),\n",
        "        name=\"summarize_text\",\n",
        "        description=\"\"\"Summarize a piece of text into key points.\n",
        "            Use this to condense long paper content into digestible summaries.\"\"\",\n",
        "        args_schema=SummarizeTextInput\n",
        "    )\n",
        "\n",
        "    return [search_arxiv_tool, download_paper_tool, summarize_text_tool]\n",
        "\n",
        "# ---- Agent Setup ----\n",
        "\n",
        "def create_research_agent(openai_api_key: str, groq_api_key: str) -> AgentExecutor:\n",
        "    \"\"\"Create a research agent using LangChain's AgentExecutor.\"\"\"\n",
        "\n",
        "    # Create the LLM\n",
        "    llm = ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\")\n",
        "\n",
        "    # Create the structured tools\n",
        "    tools = create_structured_tools(groq_api_key)\n",
        "\n",
        "    # Create the agent prompt\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a helpful AI research assistant. You have access to tools for searching academic papers,\n",
        "            downloading paper content, and summarizing text.\n",
        "\n",
        "            Your approach should be:\n",
        "            1. Understand the research question\n",
        "            2. Search for relevant papers using appropriate keywords\n",
        "            3. Download and analyze the most relevant papers\n",
        "            4. Synthesize information to answer the question\n",
        "\n",
        "            Think step by step and make decisions about which tools to use when.\n",
        "            When you've gathered enough information, provide a comprehensive answer.\"\"\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ])\n",
        "\n",
        "    # Create the agent\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "    # Create agent executor\n",
        "    return AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=10)\n",
        "\n",
        "# ---- Custom Agent with Decision Logging ----\n",
        "\n",
        "class ResearchAgentWithDecisions(AgentExecutor):\n",
        "    \"\"\"Extended AgentExecutor that logs decision points for educational purposes.\"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        object.__setattr__(self, 'decision_log', [])\n",
        "\n",
        "    def _call(self, inputs: Dict[str, Any], *args, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"Override _call to log decision points.\"\"\"\n",
        "\n",
        "        print(\"🤖 RESEARCH AGENT INITIATED\")\n",
        "        print(f\"📋 Query: {inputs.get('input', '')}\")\n",
        "\n",
        "        # Call the parent class method\n",
        "        result = super()._call(inputs, *args, **kwargs)\n",
        "\n",
        "        print(\"\\n✨ AGENT EXECUTION COMPLETE\")\n",
        "        print(\"\\n----- DECISION LOG -----\")\n",
        "        for i, decision in enumerate(self.decision_log):\n",
        "            print(f\"{i+1}. {decision}\")\n",
        "        print(\"--------------------------\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _take_next_step(self, name_to_tool_map: Dict[str, Any], color_mapping: Dict[str, str],\n",
        "                       inputs: Dict[str, Any], intermediate_steps: List[tuple], *args, **kwargs) -> Any:\n",
        "        \"\"\"Override to log agent decisions.\"\"\"\n",
        "\n",
        "        try:\n",
        "            result = super()._take_next_step(name_to_tool_map, color_mapping, inputs, intermediate_steps, *args, **kwargs)\n",
        "\n",
        "            # Log the decision\n",
        "            if isinstance(result, list):\n",
        "                for r in result:\n",
        "                    if isinstance(r, tuple) and isinstance(r[0], AgentAction):\n",
        "                        action = r[0]\n",
        "                        decision = f\"Decided to use tool: {action.tool} with input: {action.tool_input}\"\n",
        "                        self.decision_log.append(decision)\n",
        "                        print(f\"\\n🔄 Agent Decision: {decision}\")\n",
        "            elif isinstance(result, AgentFinish):\n",
        "                decision = f\"Decided to finish with output: {result.return_values.get('output', '')[:100]}...\"\n",
        "                self.decision_log.append(decision)\n",
        "                print(f\"\\n✅ Agent Finish: {decision}\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in agent execution: {e}\")\n",
        "            return AgentFinish(return_values={\"output\": \"An error occurred during execution.\"}, log=\"Error\")\n",
        "\n",
        "# ---- Usage Example ----\n",
        "\n",
        "def run_research_query(query: str, openai_api_key: str, groq_api_key: str) -> str:\n",
        "    \"\"\"Run a research query using the agent.\"\"\"\n",
        "\n",
        "    # Create the research agent\n",
        "    agent_executor = ResearchAgentWithDecisions(\n",
        "        agent=create_openai_tools_agent(\n",
        "            ChatOpenAI(api_key=openai_api_key, model=\"o3-mini\"),\n",
        "            create_structured_tools(groq_api_key),\n",
        "            ChatPromptTemplate.from_messages([\n",
        "                (\"system\", \"\"\"You are a helpful AI research assistant. You have access to tools for searching academic papers,\n",
        "                    downloading paper content, and summarizing text.\n",
        "\n",
        "                    Your approach should be:\n",
        "                    1. Understand the research question\n",
        "                    2. Search for relevant papers using appropriate keywords\n",
        "                    3. Download and analyze the most relevant papers\n",
        "                    4. Synthesize information to answer the question\n",
        "\n",
        "                    Think step by step and make decisions about which tools to use when.\n",
        "                    When you've gathered enough information, provide a comprehensive answer.\"\"\"),\n",
        "                (\"human\", \"{input}\"),\n",
        "                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "            ])\n",
        "        ),\n",
        "        tools=create_structured_tools(groq_api_key),\n",
        "        verbose=True,\n",
        "        max_iterations=10\n",
        "    )\n",
        "\n",
        "    # Execute the query\n",
        "    result = agent_executor.invoke({\"input\": query})\n",
        "\n",
        "    return result[\"output\"]\n",
        "\n",
        "# ---- Example usage ----\n",
        "def run_example(passed_query=\"\"):\n",
        "    # Load API keys from environment\n",
        "    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not groq_api_key or not openai_api_key:\n",
        "        raise ValueError(\"API keys not found. Make sure GROQ_API_KEY and OPENAI_API_KEY are set.\")\n",
        "\n",
        "    # Example query\n",
        "    # query = \"What are the latest advancements in quantum error correction?\"\n",
        "    query = \"What some progound insights in the best papers covering fine tuning in LLMs?\"\n",
        "    if passed_query:\n",
        "        query = passed_query\n",
        "\n",
        "    # Run the query\n",
        "    response = run_research_query(query, openai_api_key, groq_api_key)\n",
        "\n",
        "    print(\"\\n----- FINAL ANSWER -----\")\n",
        "    print(response)\n",
        "    print(\"\\n--------------------------\")\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0J5x_7GjkyZN",
        "outputId": "1517e102-a165-4294-9e29-013b9dc82b3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ResearchAgentWithDecisions chain...\u001b[0m\n",
            "🤖 RESEARCH AGENT INITIATED\n",
            "📋 Query: What some progound insights in the best papers covering fine tuning in LLMs?\n",
            "\u001b[36;1m\u001b[1;3m[{'id': '2403.09162v1', 'title': 'Unveiling the Generalization Power of Fine-Tuned Large Language Models', 'authors': ['Haoran Yang', 'Yumeng Zhang', 'Jiaqi Xu', 'Hongyuan Lu', 'Pheng Ann Heng', 'Wai Lam'], 'url': 'http://arxiv.org/pdf/2403.09162v1'}, {'id': '2309.13192v2', 'title': 'Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation', 'authors': ['Kai Huang', 'Hanyun Yin', 'Heng Huang', 'Wei Gao'], 'url': 'http://arxiv.org/pdf/2309.13192v2'}, {'id': '2310.10047v1', 'title': 'Improving Large Language Model Fine-tuning for Solving Math Problems', 'authors': ['Yixin Liu', 'Avi Singh', 'C. Daniel Freeman', 'John D. Co-Reyes', 'Peter J. Liu'], 'url': 'http://arxiv.org/pdf/2310.10047v1'}, {'id': '2408.13296v3', 'title': 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities', 'authors': ['Venkatesh Balavadhani Parthasarathy', 'Ahtsham Zafar', 'Aafaq Khan', 'Arsalan Shahid'], 'url': 'http://arxiv.org/pdf/2408.13296v3'}, {'id': '2309.08859v1', 'title': 'Rethinking Learning Rate Tuning in the Era of Large Language Models', 'authors': ['Hongpeng Jin', 'Wenqi Wei', 'Xuyu Wang', 'Wenbin Zhang', 'Yanzhao Wu'], 'url': 'http://arxiv.org/pdf/2309.08859v1'}]\u001b[0m\n",
            "🔄 Agent Decision: Decided to use tool: search_arxiv with input: {'query': 'fine tuning in LLMs profound insights fine tuning best papers on fine tuning large language models'}\n",
            "\u001b[33;1m\u001b[1;3mThe Ultimate Guide to Fine-Tuning LLMs from\n",
            "Basics to Breakthroughs: An Exhaustive Review of\n",
            "Technologies, Research, Best Practices, Applied\n",
            "Research Challenges and Opportunities\n",
            "(Version 1.1)\n",
            "Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, and\n",
            "Arsalan Shahid\n",
            "@ CeADAR Connect Group\n",
            "CeADAR: Ireland’s Centre for AI, University College Dublin, Belfield, Dublin, Ireland\n",
            "{venkatesh.parthasarathy, ahtsham.zafar, aafaq.khan, arsalan.shahid }@ ucd.ie\n",
            "October 2024arXiv:2408.13296v3  [cs.LG]  30 Oct 2024Abstract\n",
            "This technical report thoroughly examines the process of fine-tuning Large Language Models (LLMs),\n",
            "integrating theoretical insights and practical applications. It begins by tracing the historical develop-\n",
            "ment of LLMs, emphasising their evolution from traditional Natural Language Processing (NLP) models\n",
            "and their pivotal role in modern AI systems. The analysis differentiates between various fine-tuning\n",
            "methodologies, including supervised, unsupervised, and instruction-based approaches, underscoring their\n",
            "respective implications for specific tasks.\n",
            "A structured seven-stage pipeline for LLM fine-tuning is introduced, covering the complete lifecycle\n",
            "from data preparation to model deployment. Key considerations include data collection strategies,\n",
            "handling of imbalanced datasets, model initialisation, and optimisation techniques, with a particular\n",
            "focus on hyperparameter tuning. The report also highlights parameter-efficient fine-tuning methods\n",
            "such as Low-Rank Adaptation (LoRA) and Half Fine-Tuning, which balance resource constraints with\n",
            "optimal model performance.\n",
            "The exploration extends to advanced fine-tuning techniques and configurations like memory fine-\n",
            "tuning, Mixture of Experts (MoE) and Mixture of Agents (MoA), demonstrating how these methods\n",
            "harness specialised networks and multi-agent collaboration for improved outcomes. Proximal Policy\n",
            "Optimisation (PPO) and Direct Preference Optimisation (DPO) are discussed as innovative approaches\n",
            "to aligning models with human preferences, while the benefits of pruning and routing optimisations are\n",
            "examined for enhancing efficiency.\n",
            "In the latter sections, the report delves into validation frameworks, post-deployment monitoring, and\n",
            "optimisation techniques for inference. It also addresses the deployment of LLMs on distributed and\n",
            "cloud-based platforms. Additionally, cutting-edge topics such as multimodal LLMs and fine-tuning for\n",
            "audio and speech processing are covered, alongside emerging challenges related to scalability, privacy,\n",
            "and accountability.\n",
            "This report aims to serve as a comprehensive guide for researchers and practitioners, offering action-\n",
            "able insights into fine-tuning LLMs while navigating the challenges and opportunities inherent in this\n",
            "rapidly evolving field.Contents\n",
            "1 Introduction 6\n",
            "1.1 Background of Large Language Models (LLMs) . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
            "1.2 Historical Development and Key Milestones . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
            "1.3 Evolution from Traditional NLP Models to State-of-the-Art LLMs . . . . . . . . . . . . . 6\n",
            "1.3.1 Statistical Language Models (SLMs) . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
            "1.3.2 Neural Language Models (NLMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
            "1.3.3 Pre-trained Language Models (PLMs) . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
            "1.3.4 Large Language Models (LLMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
            "1.4 Overview of Current Leading LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
            "1.5 What is Fine-Tuning? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
            "1.6 Types of LLM Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
            "1.6.1 Unsupervised Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
            "1.6.2 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
            "1.6.3 Instruction Fine-Tuning via Prompt Engineering . . . . . . . . . . . . . . . . . . . 10\n",
            "1.7 Pre-training vs Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n",
            "1.8 Importance of Fine-Tuning LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n",
            "1.9 Retrieval Augmented Generation (RAG) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
            "1.9.1 Traditional RAG Pipeline and Steps . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
            "1.9.2 Benefits of Using RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
            "1.9.3 Challenges and Considerations in Serving RAG . . . . . . . . . . . . . . . . . . . . 12\n",
            "1.9.4 Use Cases and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
            "1.9.5 Considerations for Choosing Between RAG and Fine-Tuning . . . . . . . . . . . . 12\n",
            "1.10 Objectives of the Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n",
            "1.10.1 Goals and Scope . . . . . . . . . . . .\u001b[0m\n",
            "🔄 Agent Decision: Decided to use tool: download_paper with input: {'paper_id': '2408.13296v3'}\n",
            "\n",
            "✅ Agent Finish: Decided to finish with output: I reviewed some of the influential recent works on fine-tuning in large language models (LLMs), and ...\n",
            "\n",
            "✨ AGENT EXECUTION COMPLETE\n",
            "\n",
            "----- DECISION LOG -----\n",
            "1. Decided to use tool: search_arxiv with input: {'query': 'fine tuning in LLMs profound insights fine tuning best papers on fine tuning large language models'}\n",
            "2. Decided to use tool: download_paper with input: {'paper_id': '2408.13296v3'}\n",
            "3. Decided to finish with output: I reviewed some of the influential recent works on fine-tuning in large language models (LLMs), and ...\n",
            "--------------------------\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "----- FINAL ANSWER -----\n",
            "I reviewed some of the influential recent works on fine-tuning in large language models (LLMs), and several profound insights have emerged across these papers. Here are some of the key takeaways:\n",
            "\n",
            "1. Deep Understanding of the Fine-Tuning Pipeline:\n",
            "   • Many of the best papers emphasize that fine-tuning is not just “tweaking” a model’s weights but involves a comprehensive, multi-stage pipeline—from careful data preparation and handling imbalanced datasets to model initialization, hyperparameter tuning, and validation. For example, in “The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs,” a seven-stage pipeline is introduced that underscores the importance of each step in ensuring robust performance.\n",
            "\n",
            "2. Parameter-Efficient Fine-Tuning:\n",
            "   • With the growing size of LLMs, the cost and computational burden of fine-tuning a full model becomes prohibitively high. As a result, techniques like Low-Rank Adaptation (LoRA) and Half Fine-Tuning have gained traction. These approaches allow for effective adaptation to new tasks while altering far fewer parameters, thus saving on both compute costs and time, and even reducing memory footprint without sacrificing much in performance.\n",
            "\n",
            "3. Advanced Techniques for Task Specialization:\n",
            "   • Beyond the standard supervised or instruction-based fine-tuning approaches, researchers are exploring advanced configurations. For example, integrating memory fine-tuning or methods inspired by mixtures-of-experts (MoE, MoA) can enable models to manage specialized sub-tasks with dedicated “mini-models” or agents within the overall architecture. This multi-faceted approach helps in focusing on particular tasks or domains, improving both accuracy and efficiency.\n",
            "\n",
            "4. Alignment with Human Preferences:\n",
            "   • Fine-tuning is increasingly seen as a way to align model outputs with human values or preferences. This is evident in approaches like Reinforcement Learning from Human Feedback (RLHF) as well as methods such as Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO). These strategies help adjust the model’s behavior post-pretraining to make its responses more useful, safe, or contextually appropriate for particular applications.\n",
            "\n",
            "5. Sustainability and Efficiency Considerations:\n",
            "   • As energy consumption and environmental impact become more critical, there’s a growing focus on “Green AI.” One recent work, “Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation,” explores adaptive update schemes that reduce redundant computations and energy usage during fine-tuning. This trend underscores the need for balancing performance gains with sustainability and cost-effectiveness.\n",
            "\n",
            "6. Benchmarking and Generalization Power:\n",
            "   • Several studies, such as “Unveiling the Generalization Power of Fine-Tuned Large Language Models,” highlight that properly fine-tuned LLMs can often generalize well on downstream tasks—even beyond the suite they were specifically tuned for. This insight shows that the careful design of fine-tuning strategies not only boosts task-specific performance but also preserves or even improves the model's overall robustness.\n",
            "\n",
            "7. Integration with Retrieval Augmented Generation (RAG):\n",
            "   • There is also an interesting discussion about how fine-tuning interacts with other techniques like Retrieval Augmented Generation (RAG). While standard fine-tuning customizes the entire network, RAG incorporates external knowledge sources dynamically. Papers highlight that in some applications, a hybrid approach may yield better performance, as it leverages both the adaptability of fine-tuning and the up-to-date information that retrieval mechanisms provide.\n",
            "\n",
            "Overall, these insights reflect a maturing field where fine-tuning is evolving from a relatively “brute force” approach into a more nuanced set of strategies that incorporate efficiency, specialized adaptation, and alignment to human values. The trend is clearly moving towards methods that are not only effective in improving task performance but that also account for computational cost, sustainability, and the broader implications of deploying such large-scale models in diverse real-world settings.\n",
            "\n",
            "--------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I reviewed some of the influential recent works on fine-tuning in large language models (LLMs), and several profound insights have emerged across these papers. Here are some of the key takeaways:\\n\\n1. Deep Understanding of the Fine-Tuning Pipeline:\\n   • Many of the best papers emphasize that fine-tuning is not just “tweaking” a model’s weights but involves a comprehensive, multi-stage pipeline—from careful data preparation and handling imbalanced datasets to model initialization, hyperparameter tuning, and validation. For example, in “The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs,” a seven-stage pipeline is introduced that underscores the importance of each step in ensuring robust performance.\\n\\n2. Parameter-Efficient Fine-Tuning:\\n   • With the growing size of LLMs, the cost and computational burden of fine-tuning a full model becomes prohibitively high. As a result, techniques like Low-Rank Adaptation (LoRA) and Half Fine-Tuning have gained traction. These approaches allow for effective adaptation to new tasks while altering far fewer parameters, thus saving on both compute costs and time, and even reducing memory footprint without sacrificing much in performance.\\n\\n3. Advanced Techniques for Task Specialization:\\n   • Beyond the standard supervised or instruction-based fine-tuning approaches, researchers are exploring advanced configurations. For example, integrating memory fine-tuning or methods inspired by mixtures-of-experts (MoE, MoA) can enable models to manage specialized sub-tasks with dedicated “mini-models” or agents within the overall architecture. This multi-faceted approach helps in focusing on particular tasks or domains, improving both accuracy and efficiency.\\n\\n4. Alignment with Human Preferences:\\n   • Fine-tuning is increasingly seen as a way to align model outputs with human values or preferences. This is evident in approaches like Reinforcement Learning from Human Feedback (RLHF) as well as methods such as Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO). These strategies help adjust the model’s behavior post-pretraining to make its responses more useful, safe, or contextually appropriate for particular applications.\\n\\n5. Sustainability and Efficiency Considerations:\\n   • As energy consumption and environmental impact become more critical, there’s a growing focus on “Green AI.” One recent work, “Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation,” explores adaptive update schemes that reduce redundant computations and energy usage during fine-tuning. This trend underscores the need for balancing performance gains with sustainability and cost-effectiveness.\\n\\n6. Benchmarking and Generalization Power:\\n   • Several studies, such as “Unveiling the Generalization Power of Fine-Tuned Large Language Models,” highlight that properly fine-tuned LLMs can often generalize well on downstream tasks—even beyond the suite they were specifically tuned for. This insight shows that the careful design of fine-tuning strategies not only boosts task-specific performance but also preserves or even improves the model's overall robustness.\\n\\n7. Integration with Retrieval Augmented Generation (RAG):\\n   • There is also an interesting discussion about how fine-tuning interacts with other techniques like Retrieval Augmented Generation (RAG). While standard fine-tuning customizes the entire network, RAG incorporates external knowledge sources dynamically. Papers highlight that in some applications, a hybrid approach may yield better performance, as it leverages both the adaptability of fine-tuning and the up-to-date information that retrieval mechanisms provide.\\n\\nOverall, these insights reflect a maturing field where fine-tuning is evolving from a relatively “brute force” approach into a more nuanced set of strategies that incorporate efficiency, specialized adaptation, and alignment to human values. The trend is clearly moving towards methods that are not only effective in improving task performance but that also account for computational cost, sustainability, and the broader implications of deploying such large-scale models in diverse real-world settings.\""
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_example(passed_query=\"What some profound insights in the best papers covering fine tuning in LLMs?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4MZwaRkbVcNw",
        "SmeBWd33Vekn",
        "uZkflG4RVo-8",
        "6oyNDKE2V3ZT",
        "ufDX-MpdPr93",
        "rZgHyK06i51-",
        "WgZcTuQapDSP",
        "mwMNfyscpHjT",
        "vbsouuK6q3LA",
        "uIP5opPY0pTM",
        "GOaAIXOE4_RH"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
